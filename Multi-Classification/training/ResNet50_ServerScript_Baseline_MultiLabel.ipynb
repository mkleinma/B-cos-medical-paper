{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 88\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Set random seeds for reproducibility\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m     89\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m     90\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(args\u001b[38;5;241m.\u001b[39mseed)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import pickle \n",
    "import os\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import csv\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "from libraries import augmentations\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", type=int, required=True, help=\"Random seed for training\")\n",
    "parser.add_argument(\"--augmentation\", type=str, choices=[\"no\", \"light\", \"heavy\"], required=True, help=\"Augmentation Type\")\n",
    "parser.add_argument(\"--sampling\", type=lambda x: x.lower() == \"true\", default=False, help=\"Enable sampling (True/False)\")\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "## assisting script\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, fold, path, best_f1, best_recall):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'fold': fold,\n",
    "        'best_f1': best_f1,\n",
    "        'best_recall' : best_recall\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved at {path}\")\n",
    "    \n",
    "    \n",
    "def load_checkpoint(path, model, optimizer, scheduler):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        fold = checkpoint['fold']  # Load the last completed fold\n",
    "        best_f1 = checkpoint['best_f1']\n",
    "        best_recall = checkpoint['best_recall']\n",
    "        print(f\"Checkpoint loaded from {path}\")\n",
    "        return start_epoch, fold, best_f1, best_recall, True\n",
    "    return 0, 0, 0.0, 0.0, False\n",
    "\n",
    "def find_latest_checkpoint(model_output_dir):\n",
    "    \"\"\"\n",
    "    Find the latest available checkpoint in the directory.\n",
    "    Looks for checkpoints named in the format 'checkpoint_fold_X.pth' where X is the fold number.\n",
    "    \"\"\"\n",
    "    for fold in range(5, 0, -1):  # Check from fold_5 to fold_1\n",
    "        checkpoint_path = os.path.join(model_output_dir, f\"checkpoint_fold_{fold}.pth\")\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            print(f\"Found checkpoint: {checkpoint_path}\")\n",
    "            return checkpoint_path, fold\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "samp_text = \"nosamp\"\n",
    "if args.sampling:\n",
    "    samp_text = \"oversamp\"\n",
    "    \n",
    "# Paths\n",
    "csv_path = r\"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/training_splits/multilabel_dataset.csv\"\n",
    "image_folder = r\"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/vinbigdata-chest-xray-abnormalities-detection/train\"\n",
    "splits_path = r\"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/training_splits/vinbigdata_5fold_splits.pkl\"\n",
    "model_output_dir = f\"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/trained_models_multilabel/30_base_resnet50_{args.augmentation}_{samp_text}/seed_{args.seed}\"\n",
    "cm_output_dir = os.path.join(model_output_dir, \"confusion_matrix\")\n",
    "\n",
    "\n",
    "# to make sure there is no issue when paths dont exist\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "# Load data and splits\n",
    "data = pd.read_csv(csv_path)\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "\n",
    "# Dataset class for Pneumonia\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.data.iloc[idx, 0]\n",
    "        labels = self.data.iloc[idx, 1:].values.astype('float32')\n",
    "        image_path = os.path.join(self.image_folder, f\"{image_id}.dcm\")\n",
    "        \n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(labels)\n",
    "\n",
    "\n",
    "\n",
    "# Define transformations for the datasets --- resize for baselines as model.transform else resizes\n",
    "if args.augmentation == \"no\":\n",
    "    transform = augmentations.get_no_augmentations_resize()\n",
    "elif args.augmentation == \"light\":\n",
    "    transform = augmentations.get_light_augmentations_resize()\n",
    "elif args.augmentation == \"heavy\":\n",
    "    transform = augmentations.get_heavy_augmentations_no_rotation_resize()\n",
    "\n",
    "transform_val = augmentations.get_no_augmentations_resize()\n",
    "\n",
    "\n",
    "fold = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training loop\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, 14)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "start_epoch, start_fold, best_f1, best_recall, checkpoint_exists = 0, 0, 0.0, 0.0, False\n",
    "\n",
    "# we check for latest checkpoint and if it exists, then we load the checkpoint and start from there - else we start from 0 and it does not exist\n",
    "latest_checkpoint_path, latest_fold = find_latest_checkpoint(model_output_dir)\n",
    "if latest_checkpoint_path:\n",
    "    start_epoch, start_fold, best_f1, best_recall, checkpoint_exists = load_checkpoint(\n",
    "        latest_checkpoint_path, model, optimizer, scheduler)\n",
    "\n",
    "for current_fold, (train_idx, val_idx) in enumerate(splits):\n",
    "    fold = current_fold + 1\n",
    "    print(f\"Training fold {fold}...\")\n",
    "    \n",
    "    if checkpoint_exists and fold < start_fold:\n",
    "        print(f\"Skipping fold {current_fold} as it's already completed.\")\n",
    "        continue  # Skip completed folds\n",
    "            \n",
    "    # if we dont start from checkpoint: initialize new model to train\n",
    "    if not checkpoint_exists or current_fold != start_fold:\n",
    "        best_f1 = 0.0\n",
    "        best_recall = 0.0\n",
    "        model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 14) \n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        model = model.to(device)\n",
    "        checkpoint_exists = False\n",
    "\n",
    "\n",
    "    log_dir = os.path.join(model_output_dir, f\"tensorboard_logs_fold_{fold}\")\n",
    "    log_writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    # Prepare datasets and dataloaders\n",
    "    train_data = data.iloc[train_idx]\n",
    "    val_data = data.iloc[val_idx]\n",
    "\n",
    "    train_dataset = MultiLabelDataset(train_data, image_folder, transform=transform)\n",
    "    val_dataset = MultiLabelDataset(val_data, image_folder, transform=transform_val)\n",
    "    \n",
    "    \n",
    "    if args.sampling:\n",
    "        label_counts = train_data.iloc[:, 1:].sum(axis=0).values # how many of each label\n",
    "        class_weights = 1. / (label_counts + 0.1) # smoothing factor 0.1\n",
    "        class_weights = np.clip(class_weights, a_min = 0.01, a_max = 10.0)\n",
    "        sample_weights = train_data.iloc[:, 1:].dot(class_weights)\n",
    "        sample_weights /= sample_weights.max()\n",
    "        sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  \n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "            \n",
    "    # Train for the current fold\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        if (epoch < start_epoch and start_epoch < num_epochs):\n",
    "            print(f\"Skipping epoch {epoch}, resuming from checkpoint at epoch {start_epoch}.\")\n",
    "            continue\n",
    "\n",
    "        if fold == start_fold:\n",
    "            checkpoint_exists = False \n",
    "            start_epoch = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            probs = torch.sigmoid(outputs)  # Convert logits to probabilities\n",
    "            preds = (probs > 0.5).int()  # Multi-label thresholding at 0.5\n",
    "            \n",
    "            # per label accuracy\n",
    "            correct = (preds == labels).sum(dim=1) == labels.shape[1]  # Check all labels per sample\n",
    "            train_accuracy = correct.float().mean().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct / total\n",
    "        \n",
    "        log_writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "        log_writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n",
    "\n",
    "\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                probs = torch.sigmoid(outputs)  # Convert to probabilities\n",
    "                preds = (probs > 0.5).int()\n",
    "\n",
    "                all_probs.extend(probs.cpu().numpy().flatten())\n",
    "                all_preds.extend(preds.cpu().numpy().flatten())\n",
    "                all_labels.extend(labels.cpu().numpy().flatten())\n",
    "                \n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        recall = recall_score(all_labels, all_preds, average='macro')\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        auc = roc_auc_score(all_labels, all_probs, average='macro')\n",
    "        \n",
    "        log_writer.add_scalar('Loss/Validation', val_loss, epoch+1)\n",
    "        log_writer.add_scalar('Accuracy/Validation', val_accuracy, epoch+1)\n",
    "        log_writer.add_scalar('Metrics/Precision', precision, epoch+1)\n",
    "        log_writer.add_scalar('Metrics/Recall', recall, epoch+1)\n",
    "        log_writer.add_scalar('Metrics/F1', f1, epoch+1)\n",
    "        log_writer.add_scalar('Metrics/AUC', auc, epoch+1)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        log_writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
    "        \n",
    "        num_classes = all_labels.shape[1]  # 14 in your case\n",
    "        cm_per_class = np.zeros((num_classes, 2, 2))  # 14 confusion matrices of size 2x2\n",
    "\n",
    "        for class_idx in range(num_classes):\n",
    "            cm_per_class[class_idx] = confusion_matrix(\n",
    "                all_labels[:, class_idx], all_preds[:, class_idx], labels=[0, 1]\n",
    "            )\n",
    "\n",
    "        # Save per-class confusion matrices\n",
    "        cm_file_path = os.path.join(cm_output_dir, f\"confusion_matrices.json\")\n",
    "        with open(cm_file_path, 'w') as cm_file:\n",
    "            json.dump({'confusion_matrices': cm_per_class.tolist()}, cm_file, indent=4)\n",
    "\n",
    "        print(f\"Per-class confusion matrices saved at {cm_file_path}\")\n",
    "        \n",
    "        for class_idx in range(num_classes):\n",
    "            cm = cm_per_class[class_idx]\n",
    "            class_name = f\"Class_{class_idx}\"\n",
    "            print(f\"Logging confusion matrix for {class_name}...\")\n",
    "\n",
    "            # Plot confusion matrix\n",
    "            cm_figure = plot_confusion_matrix(cm, ['Class 0', 'Class 1'])\n",
    "\n",
    "            # Log the plot to TensorBoard\n",
    "            log_writer.add_figure(f'Confusion_Matrix/{class_name}', cm_figure, epoch)\n",
    "\n",
    "        if (recall > best_recall):\n",
    "            best_recall = recall\n",
    "            torch.save(model.state_dict(), os.path.join(model_output_dir, f\"pneumonia_detection_model_resnet_baseline_bestrecall_{fold}.pth\"))\n",
    "            cm_file_path = os.path.join(cm_output_dir, f\"confusion_matrix_best_recall_{fold}.json\")\n",
    "            with open(cm_file_path, 'w') as cm_file:\n",
    "                json.dump({'confusion_matrix': cm_per_class.tolist()}, cm_file, indent=4)\n",
    "            print(f\"Confusion Matrix for Fold {fold} saved at {cm_file_path}\")\n",
    "\n",
    "        \n",
    "        if (f1 > best_f1):\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), os.path.join(model_output_dir, f\"pneumonia_detection_model_resnet_baseline_bestf1_{fold}.pth\"))\n",
    "            cm_file_path = os.path.join(cm_output_dir, f\"confusion_matrix_best_f1_{fold}.json\")\n",
    "            with open(cm_file_path, 'w') as cm_file:\n",
    "                json.dump({'confusion_matrix': cm_per_class.tolist()}, cm_file, indent=4)\n",
    "            print(f\"Confusion Matrix for Fold {fold} saved at {cm_file_path}\")\n",
    "            \n",
    "        save_checkpoint_path = os.path.join(model_output_dir, f\"checkpoint_fold_{fold}.pth\")\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, fold, save_checkpoint_path, best_f1, best_recall)\n",
    "\n",
    "        print(f\"Fold {fold}, Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                f\"Val Acc: {val_accuracy:.4f}, Precision: {precision:.4f}, \"\n",
    "                f\"Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    print(f\"Finished training fold {fold}.\\n\")\n",
    "    \n",
    "    # Save the final model\n",
    "    model_path = f\"pneumonia_detection_model_fold_{fold}_resnet_baseline.pth\"\n",
    "    torch.save(model.state_dict(), os.path.join(model_output_dir, model_path))\n",
    "    log_writer.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
