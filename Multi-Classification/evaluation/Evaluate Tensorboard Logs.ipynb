{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Evaluation of Logs\n",
    "Tensorboard logs play a huge role in evaluating our model performance\n",
    "This script focuses on getting the necessary information from a log directory to have educated information on the model performance.\n",
    "\n",
    "The first cell (directly below) focuses on deriving the best F1-Score and Recall model\n",
    "The second cell focuses on deriving from a directory of several folds (considering this project uses five-fold cross validation) the average across all folds of the necessary metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scalars: ['Loss/Train', 'Accuracy/Train', 'Loss/Validation', 'Accuracy/Validation', 'Metrics/Precision', 'Metrics/Recall', 'Metrics/F1', 'Metrics/AUC', 'Learning_Rate']\n",
      "Mean Average Precision (mAP): 0.3408434331417084\n",
      "Min Precision: 0.29667606949806213\n",
      "Max Precision: 0.42765071988105774\n",
      "\n",
      "Mean Average Recall (mAR): 0.43743633925914766\n",
      "Min Recall: 0.3488323390483856\n",
      "Max Recall: 0.48400864005088806\n",
      "\n",
      "Mean Accuracy: 0.8584404706954956\n",
      "Min Accuracy: 0.8102619051933289\n",
      "Max Accuracy: 0.881428599357605\n",
      "\n",
      "Corresponding Accuracy: 0.881428599357605 at Epoch 2\n",
      "Corresponding Precision: 0.42765071988105774\n",
      "Highest Recall: 0.43087950348854065\n",
      "Corresponding F1 Score: 0.36959731578826904\n",
      "Corresponding AUC: 0.7989233732223511\n",
      "\n",
      "\n",
      "Corresponding Accuracy: 0.8814\n",
      "Corresponding Precision: 0.4277\n",
      "Corresponding Recall: 0.4309\n",
      "Highest F1 Score: 0.3696 at Epoch 3\n",
      "Corresponding AUC: 0.7989\n",
      "\n",
      "\n",
      "Corresponding Accuracy: 0.8103\n",
      "Corresponding Precision: 0.3153\n",
      "Highest Recall: 0.484 at Epoch 5\n",
      "Corresponding F1 Score: 0.3266\n",
      "Corresponding AUC: 0.7611\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "log_dir = r\"D:\\tensorboard_logs_fold_1\"\n",
    "#log_dir = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\ResNet50_BCos_Heavy\\seed_0\\tensorboard_logs_fold_1\"\n",
    "\n",
    "event_acc = EventAccumulator(log_dir)\n",
    "event_acc.Reload()\n",
    "\n",
    "# List all available scalars\n",
    "available_tags = event_acc.Tags()[\"scalars\"]\n",
    "print(\"Available scalars:\", available_tags)\n",
    "\n",
    "# Extract scalar values per epoch\n",
    "precision_values = event_acc.Scalars(\"Metrics/Precision\")\n",
    "accuracy_values = event_acc.Scalars(\"Accuracy/Validation\")\n",
    "f1_values = event_acc.Scalars(\"Metrics/F1\")\n",
    "recall_values = event_acc.Scalars(\"Metrics/Recall\")\n",
    "auc_values = event_acc.Scalars(\"Metrics/AUC\")\n",
    "\n",
    "\n",
    "# Get epoch-wise values\n",
    "epochs = [x.step for x in precision_values]\n",
    "accuracy_scores = [x.value for x in accuracy_values]\n",
    "f1_scores = [x.value for x in f1_values]\n",
    "precision_scores = [x.value for x in precision_values]  # Convert to list\n",
    "recall_scores = [x.value for x in recall_values]  # Convert to list\n",
    "\n",
    "# Compute mean, min, max precision\n",
    "mean_precision = np.mean(precision_scores)\n",
    "minimum_precision = np.min(precision_scores)\n",
    "maximum_precision = np.max(precision_scores)\n",
    "\n",
    "print(f\"Mean Average Precision (mAP): {mean_precision}\")\n",
    "print(f\"Min Precision: {minimum_precision}\")\n",
    "print(f\"Max Precision: {maximum_precision}\")\n",
    "\n",
    "print()\n",
    "\n",
    "mean_recall = np.mean(recall_scores)\n",
    "minimum_recall = np.min(recall_scores)\n",
    "maximum_recall = np.max(recall_scores)\n",
    "\n",
    "print(f\"Mean Average Recall (mAR): {mean_recall}\")\n",
    "print(f\"Min Recall: {minimum_recall}\")\n",
    "print(f\"Max Recall: {maximum_recall}\")\n",
    "\n",
    "\n",
    "# Compute mean, min, max accuracy\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "minimum_accuracy = np.min(accuracy_scores)\n",
    "maximum_accuracy = np.max(accuracy_scores)\n",
    "\n",
    "print()\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Min Accuracy: {minimum_accuracy}\")\n",
    "print(f\"Max Accuracy: {maximum_accuracy}\")\n",
    "\n",
    "\n",
    "best_accuracy_idx = np.argmax(accuracy_scores)\n",
    "best_acc_epoch = epochs[best_accuracy_idx]\n",
    "best_acc_f1 = f1_scores[best_accuracy_idx]\n",
    "best_acc_precision = precision_values[best_accuracy_idx].value\n",
    "best_acc_recall = recall_values[best_accuracy_idx].value  \n",
    "best_acc_accuracy = accuracy_values[best_accuracy_idx].value\n",
    "best_acc_auc = auc_values[best_accuracy_idx].value\n",
    "\n",
    "print()\n",
    "print(f\"Corresponding Accuracy: {best_acc_accuracy} at Epoch {best_accuracy_idx}\")\n",
    "print(f\"Corresponding Precision: {best_acc_precision}\")\n",
    "print(f\"Highest Recall: {best_acc_recall}\")\n",
    "print(f\"Corresponding F1 Score: {best_acc_f1}\")\n",
    "print(f\"Corresponding AUC: {best_acc_auc}\")\n",
    "print()\n",
    "\n",
    "# Find best epoch based on highest F1 score\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "best_f1_epoch = epochs[best_f1_idx]\n",
    "best_f1 = f1_scores[best_f1_idx]\n",
    "best_f1_precision = precision_values[best_f1_idx].value\n",
    "best_f1_recall = recall_values[best_f1_idx].value  \n",
    "best_f1_accuracy = accuracy_values[best_f1_idx].value\n",
    "best_f1_auc = auc_values[best_f1_idx].value\n",
    "\n",
    "\n",
    "best_f1_precision = round(best_f1_precision, 4)\n",
    "best_f1_recall = round(best_f1_recall, 4)\n",
    "best_f1_accuracy = round(best_f1_accuracy, 4)\n",
    "best_f1_auc = round(best_f1_auc, 4)\n",
    "best_f1_f1 = round(best_f1, 4)\n",
    "\n",
    "print()\n",
    "print(f\"Corresponding Accuracy: {best_f1_accuracy}\")\n",
    "print(f\"Corresponding Precision: {best_f1_precision}\")\n",
    "print(f\"Corresponding Recall: {best_f1_recall}\")\n",
    "print(f\"Highest F1 Score: {best_f1_f1} at Epoch {best_f1_epoch}\")\n",
    "print(f\"Corresponding AUC: {best_f1_auc}\")\n",
    "\n",
    "best_recall_idx = np.argmax(recall_scores)\n",
    "best_recall_epoch = epochs[best_recall_idx]\n",
    "best_recall_f1 = f1_scores[best_recall_idx]\n",
    "best_recall_precision = precision_values[best_recall_idx].value\n",
    "best_recall_recall = recall_values[best_recall_idx].value  \n",
    "best_recall_accuracy = accuracy_values[best_recall_idx].value\n",
    "best_recall_auc = auc_values[best_recall_idx].value\n",
    "\n",
    "\n",
    "best_recall_precision = round(best_recall_precision, 4)\n",
    "best_recall_recall = round(best_recall_recall, 4)\n",
    "best_recall_accuracy = round(best_recall_accuracy, 4)\n",
    "best_recall_auc = round(best_recall_auc, 4)\n",
    "best_recall_f1 = round(best_recall_f1, 4)\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(f\"Corresponding Accuracy: {best_recall_accuracy}\")\n",
    "print(f\"Corresponding Precision: {best_recall_precision}\")\n",
    "print(f\"Highest Recall: {best_recall_recall} at Epoch {best_recall_epoch}\")\n",
    "print(f\"Corresponding F1 Score: {best_recall_f1}\")\n",
    "print(f\"Corresponding AUC: {best_recall_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= AVERAGE METRICS OVER ALL FOLDS =======\n",
      "\n",
      "Best F1 Model:\n",
      "Mean Accuracy: 0.9505\n",
      "Mean Precision: 0.5637\n",
      "Mean Recall: 0.4036\n",
      "Mean F1 Score: 0.4475\n",
      "Mean AUC: 0.934\n",
      "\n",
      "Best Recall Model:\n",
      "Mean Accuracy: 0.9499\n",
      "Mean Precision: 0.5329\n",
      "Mean Recall: 0.4078\n",
      "Mean F1 Score: 0.4413\n",
      "Mean AUC: 0.934\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "log_base_dir = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\VinBigData\\ResNet_Baseline\\light_oversamp\\seed_0\\tensorboard_logs_fold_\"\n",
    "\n",
    "num_folds = 5  # Number of folds\n",
    "\n",
    "# Lists to store best values for each fold\n",
    "best_f1_f1, best_f1_precisions, best_f1_recalls, best_f1_accuracies, best_f1_aucs = [], [], [], [], []\n",
    "best_recall_f1, best_recall_precisions, best_recall_recalls, best_recall_accuracies, best_recall_aucs = [], [], [], [], []\n",
    "\n",
    "for fold in range(1, num_folds + 1):\n",
    "    log_dir = f\"{log_base_dir}{fold}\"\n",
    "    import os\n",
    "    if not os.path.exists(log_dir):\n",
    "        print(f\"CAREFUL {log_dir} does not exist\")\n",
    "        continue\n",
    "    \n",
    "    event_acc = EventAccumulator(log_dir)\n",
    "    event_acc.Reload()\n",
    "    \n",
    "    precision_values = event_acc.Scalars(\"Metrics/Precision\")\n",
    "    accuracy_values = event_acc.Scalars(\"Accuracy/Validation\")\n",
    "    f1_values = event_acc.Scalars(\"Metrics/F1\")\n",
    "    recall_values = event_acc.Scalars(\"Metrics/Recall\")\n",
    "    auc_values = event_acc.Scalars(\"Metrics/AUC\")\n",
    "    \n",
    "    epochs = [x.step for x in precision_values]\n",
    "    accuracy_scores = [x.value for x in accuracy_values]\n",
    "    f1_scores = [x.value for x in f1_values]\n",
    "    precision_scores = [x.value for x in precision_values]\n",
    "    recall_scores = [x.value for x in recall_values]\n",
    "    auc_scores = [x.value for x in auc_values]\n",
    "    \n",
    "    # Best F1 Score Model\n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    best_f1_f1.append(f1_scores[best_f1_idx])\n",
    "    best_f1_precisions.append(precision_scores[best_f1_idx])\n",
    "    best_f1_recalls.append(recall_scores[best_f1_idx])\n",
    "    best_f1_accuracies.append(accuracy_scores[best_f1_idx])\n",
    "    best_f1_aucs.append(auc_scores[best_f1_idx])\n",
    "    \n",
    "    # Best Recall Score Model\n",
    "    best_recall_idx = np.argmax(recall_scores)\n",
    "    best_recall_f1.append(f1_scores[best_recall_idx])\n",
    "    best_recall_precisions.append(precision_scores[best_recall_idx])\n",
    "    best_recall_recalls.append(recall_scores[best_recall_idx])\n",
    "    best_recall_accuracies.append(accuracy_scores[best_recall_idx])\n",
    "    best_recall_aucs.append(auc_scores[best_recall_idx])\n",
    "\n",
    "# Compute mean values across all folds\n",
    "def compute_mean(lst):\n",
    "    return round(np.mean(lst), 4) if lst else None\n",
    "\n",
    "print(\"\\n======= AVERAGE METRICS OVER ALL FOLDS =======\")\n",
    "print(\"\\nBest F1 Model:\")\n",
    "print(f\"Mean Accuracy: {compute_mean(best_f1_accuracies)}\")\n",
    "print(f\"Mean Precision: {compute_mean(best_f1_precisions)}\")\n",
    "print(f\"Mean Recall: {compute_mean(best_f1_recalls)}\")\n",
    "print(f\"Mean F1 Score: {compute_mean(best_f1_f1)}\")\n",
    "print(f\"Mean AUC: {compute_mean(best_f1_aucs)}\")\n",
    "\n",
    "print(\"\\nBest Recall Model:\")\n",
    "print(f\"Mean Accuracy: {compute_mean(best_recall_accuracies)}\")\n",
    "print(f\"Mean Precision: {compute_mean(best_recall_precisions)}\")\n",
    "print(f\"Mean Recall: {compute_mean(best_recall_recalls)}\")\n",
    "print(f\"Mean F1 Score: {compute_mean(best_recall_f1)}\")\n",
    "print(f\"Mean AUC: {compute_mean(best_recall_aucs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
