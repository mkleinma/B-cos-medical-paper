{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-based Pointing Game\n",
    "\n",
    "Cell directly below:\n",
    "- iterates one model (e.g. best performing fold 1 model of B-Cos ResNet50)\n",
    "- determines Energy-based pointing game of the fold fitting to the model (validation set) - first fold model focuses on validation set of first fold\n",
    "- calculates average energy-based pointing game result of the validation set\n",
    "- Results: higher proportion (result of energy-based pointing game) on correctly classified images\n",
    "\n",
    "Second Cell below:\n",
    "- prints image with corresponding explanation image\n",
    "- executes Energy-based pointing game on one image - giving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG for Positive Contributions\n",
    "value rather low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion: 0.0669\n",
      "Average Energy-Based Pointing Game Proportion of Incorrectly Classified Images: 0.0073, Count: 296\n",
      "Average Energy-Based Pointing Game Proportion of Correctly Classified Images: 0.0864, Count: 906\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''from dataset.pneumonia_dataset import PneumoniaDataset'''\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "#model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\Transformer_Conv_BCos\\no_nosamp\\seed_0\\pneumonia_detection_model_transformer_bcos_bestf1_1.pth\"\n",
    "model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\Pneumonia\\ResNet50_BCos\\light_oversamp\\seed_0\\pneumonia_detection_model_resnet_bestf1_1.pth\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "#model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "# Modify model and load model\n",
    "'''model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)        \n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "'''\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "\n",
    "\n",
    "def energy_point_game_debug(coords, contrib_map):\n",
    "    num_positive = (contrib_map > 0).sum().item()\n",
    "    num_negative = (contrib_map < 0).sum().item()\n",
    "    \n",
    "    print(f\"Bounding Box Coordinates: {coords.tolist()}\")\n",
    "    print(f\"Contribution Map Min/Max: {contrib_map.min().item()}, {contrib_map.max().item()}\")\n",
    "    print(f\"Positive Values Count: {num_positive}, Negative Values Count: {num_negative}\")\n",
    "    \n",
    "    result = energy_point_game(coords, contrib_map)\n",
    "    print(f\"Energy Point Game Score: {result}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()# Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "proportions = []\n",
    "proportions_correct = []\n",
    "proportions_incorrect = []\n",
    "count_correct = 0\n",
    "count_incorrect = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, patient_ids in val_loader:\n",
    "        #images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        six_channel_images = []\n",
    "        for img_tensor in images:\n",
    "            numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(numpy_image)\n",
    "            transformed_image = model.transform(pil_image)\n",
    "            six_channel_images.append(transformed_image)\n",
    "            \n",
    "        six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "        \n",
    "        for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "            filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "            if not filtered_rows.empty: \n",
    "                image = image[None]\n",
    "                output = model(image)\n",
    "                #print(image.shape)\n",
    "                prediction = torch.argmax(output, dim=1)\n",
    "                expl = model.explain(image)\n",
    "                #prediction = expl['prediction']\n",
    "                contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                #contribution_map = torch.clamp(contribution_map, min=0)\n",
    "                contribution_map[contribution_map<0] = 0  \n",
    "                proportion = 0.0\n",
    "                for _, row in filtered_rows.iterrows():\n",
    "                    x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                    #coordinates_list = [y, x, y + height,x + width]\n",
    "                    coordinates_list = [x, y, x + width, y + height]\n",
    "                    coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                    ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                    proportion += ebpg_result\n",
    "                    \n",
    "                proportions.append(proportion)\n",
    "                if prediction == 1:\n",
    "                    proportions_correct.append(proportion)\n",
    "                    count_correct = count_correct + 1\n",
    "                else:\n",
    "                    proportions_incorrect.append(proportion)\n",
    "                    count_incorrect = count_incorrect + 1\n",
    "                \n",
    "\n",
    "if proportions:\n",
    "    avg_proportion = sum(proportions) / len(proportions)\n",
    "    avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "    avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "    \n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion: {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid proportions were found.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG for Negative Contributions - everything aside Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion: 0.8687\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''from dataset.pneumonia_dataset import PneumoniaDataset'''\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "#model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\Transformer_Conv_BCos\\no_nosamp\\seed_0\\pneumonia_detection_model_transformer_bcos_bestf1_1.pth\"\n",
    "model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\ResNet50_BCos\\light_oversamp_nonorm\\seed_0\\pneumonia_detection_model_resnet_bcos_bestf1_1.pth\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "#model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "# Modify model and load model\n",
    "'''model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)        \n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "'''\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "\n",
    "\n",
    "def energy_point_game_debug(coords, contrib_map):\n",
    "    num_positive = (contrib_map > 0).sum().item()\n",
    "    num_negative = (contrib_map < 0).sum().item()\n",
    "    \n",
    "    print(f\"Bounding Box Coordinates: {coords.tolist()}\")\n",
    "    print(f\"Contribution Map Min/Max: {contrib_map.min().item()}, {contrib_map.max().item()}\")\n",
    "    print(f\"Positive Values Count: {num_positive}, Negative Values Count: {num_negative}\")\n",
    "    \n",
    "    result = energy_point_game(coords, contrib_map)\n",
    "    print(f\"Energy Point Game Score: {result}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "proportions = []\n",
    "proportions_correct = []\n",
    "proportions_incorrect = []\n",
    "count_correct = 0\n",
    "count_incorrect = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, patient_ids in val_loader:\n",
    "        #images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        six_channel_images = []\n",
    "        for img_tensor in images:\n",
    "            numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(numpy_image)\n",
    "            transformed_image = model.transform(pil_image)\n",
    "            six_channel_images.append(transformed_image)\n",
    "            \n",
    "        six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "        \n",
    "        for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "            filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "            if not filtered_rows.empty: \n",
    "                image = image[None]\n",
    "                output = model(image)\n",
    "                #print(image.shape)\n",
    "                prediction = torch.argmax(output, dim=1)\n",
    "                expl = model.explain(image)\n",
    "                #prediction = expl['prediction']\n",
    "                contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                contribution_map[contribution_map>0] = 0  \n",
    "                proportion = 1.0\n",
    "                for _, row in filtered_rows.iterrows():\n",
    "                    x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                    coordinates_list = [x, y, x + width, y + height]\n",
    "                    coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                    ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                    proportion = proportion - ebpg_result\n",
    "                    \n",
    "                proportions.append(proportion)                \n",
    "\n",
    "if proportions:\n",
    "    avg_proportion = sum(proportions) / len(proportions)    \n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion: {avg_proportion}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid proportions were found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1654\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0898, Count: 510\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2212, Count: 692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1551\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.1085, Count: 462\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.1841, Count: 740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1466\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0693, Count: 387\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.1832, Count: 816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 137\u001b[0m\n\u001b[0;32m    135\u001b[0m output \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m#prediction = torch.argmax(output, dim=1)\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m expl \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m prediction \u001b[38;5;241m=\u001b[39m expl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    139\u001b[0m contribution_map \u001b[38;5;241m=\u001b[39m expl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontribution_map\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:167\u001b[0m, in \u001b[0;36mBcosUtilMixin.explain\u001b[1;34m(self, in_tensor, idx, **grad2img_kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(in_tensor)  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    166\u001b[0m pred_out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 167\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpred_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# select output (logit) to explain\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# explain prediction\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from libraries.energyPointGame import energy_point_game, energy_point_game_recall\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "\n",
    "BASE_PATH = \"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/trained_models/\"\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "avg_proportions = []\n",
    "avg_proportions_incorrect = []\n",
    "avg_proportions_correct = []\n",
    "\n",
    "for fold in range(1,6):\n",
    "    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/Transformer_Conv_BCos/no_nosamp/seed_0/pneumonia_detection_model_transformer_bestf1_{fold}.pth\"\n",
    "    split = splits[fold-1] # fold selection\n",
    "    val_idx = split[1]  # Only use the validation indices from the first fold\n",
    "    val_data = data_splits.iloc[val_idx]\n",
    "    val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "    proportions = []\n",
    "    proportions_correct = []\n",
    "    proportions_incorrect = []\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    \n",
    "    model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "    model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "    #model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "    #model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, patient_ids in val_loader:\n",
    "            #images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            six_channel_images = []\n",
    "            for img_tensor in images:\n",
    "                numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil_image = Image.fromarray(numpy_image)\n",
    "                transformed_image = model.transform(pil_image)\n",
    "                six_channel_images.append(transformed_image)\n",
    "                \n",
    "            six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "            \n",
    "            \n",
    "            for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                if not filtered_rows.empty: \n",
    "                    image = image[None]\n",
    "                    output = model(image)\n",
    "                    #prediction = torch.argmax(output, dim=1)\n",
    "                    expl = model.explain(image)\n",
    "                    prediction = expl['prediction']\n",
    "                    contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                    contribution_map[contribution_map<0] = 0  \n",
    "                    proportion = 0.0\n",
    "                    for _, row in filtered_rows.iterrows():\n",
    "                        x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                        coordinates_list = [x, y, x + width, y + height]\n",
    "                        coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                        ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                        proportion += ebpg_result\n",
    "                        \n",
    "                    proportions.append(proportion)\n",
    "                    if prediction == 1:\n",
    "                        proportions_correct.append(proportion)\n",
    "                        count_correct = count_correct + 1\n",
    "                    else:\n",
    "                        proportions_incorrect.append(proportion)\n",
    "                        count_incorrect = count_incorrect + 1\n",
    "    if proportions:\n",
    "        avg_proportion = sum(proportions) / len(proportions)\n",
    "        avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "        avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "        avg_proportions.append(avg_proportion)\n",
    "        avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "        avg_proportions_correct.append(avg_proportion_correct)\n",
    "\n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "final_avg_prop = sum(avg_proportions) / len(avg_proportions) \n",
    "final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "\n",
    "final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "print()\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG (Negative) outside of Bounding Boxes across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Negative): 0.9708\n",
      "Average Energy-Based Pointing Game Proportion (Negative): 0.9728\n",
      "Average Energy-Based Pointing Game Proportion (Negative): 0.9553\n",
      "Average Energy-Based Pointing Game Proportion (Negative): 0.9663\n",
      "Average Energy-Based Pointing Game Proportion (Negative): 0.947\n",
      "Average Energy-Based Pointing Game Proportion (Negative): 0.9027\n",
      "Average Energy-Based Pointing Game Proportion (Negative): 0.8903\n",
      "Average Energy-Based Pointing Game Proportion (Negative): 0.8695\n",
      "Average Energy-Based Pointing Game Proportion (Negative): 0.8512\n",
      "Average Energy-Based Pointing Game Proportion (Negative): 0.8505\n",
      "Average Energy-Based Pointing Game Proportion (Negative): 0.8597\n",
      "Average Energy-Based Pointing Game Proportion (Negative): 0.8697\n",
      "Average Energy-Based Pointing Game Proportion (Negative): 0.8569\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 130\u001b[0m\n\u001b[0;32m    128\u001b[0m output \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[0;32m    129\u001b[0m prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m expl \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m#prediction = expl['prediction']\u001b[39;00m\n\u001b[0;32m    132\u001b[0m contribution_map \u001b[38;5;241m=\u001b[39m expl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontribution_map\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:177\u001b[0m, in \u001b[0;36mBcosUtilMixin.explain\u001b[1;34m(self, in_tensor, idx, **grad2img_kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         to_be_explained_logit \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m, idx]\n\u001b[0;32m    175\u001b[0m         result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplained_class_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m--> 177\u001b[0m     \u001b[43mto_be_explained_logit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43min_tensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# get weights and contribution map\u001b[39;00m\n\u001b[0;32m    180\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamic_linear_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m in_tensor\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''from dataset.pneumonia_dataset import PneumoniaDataset'''\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "avg_proportions = []\n",
    "avg_proportions_incorrect = []\n",
    "avg_proportions_correct = []\n",
    "\n",
    "for fold in range(1,6):\n",
    "    proportions = []\n",
    "    proportions_correct = []\n",
    "    proportions_incorrect = []\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_BCos/light_oversamp_nonorm/seed_0/pneumonia_detection_model_resnet_bcos_bestf1_1.pth\" \n",
    "    model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "    model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, patient_ids in val_loader:\n",
    "            #images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            six_channel_images = []\n",
    "            for img_tensor in images:\n",
    "                numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil_image = Image.fromarray(numpy_image)\n",
    "                transformed_image = model.transform(pil_image)\n",
    "                six_channel_images.append(transformed_image)\n",
    "                \n",
    "            six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "            \n",
    "            for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                if not filtered_rows.empty: \n",
    "                    image = image[None]\n",
    "                    output = model(image)\n",
    "                    prediction = torch.argmax(output, dim=1)\n",
    "                    expl = model.explain(image)\n",
    "                    #prediction = expl['prediction']\n",
    "                    contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                    contribution_map[contribution_map>0] = 0  \n",
    "                    proportion = 1.0\n",
    "                    for _, row in filtered_rows.iterrows():\n",
    "                        x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                        coordinates_list = [x, y, x + width, y + height]\n",
    "                        coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                        ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                        proportion = proportion - ebpg_result\n",
    "                        \n",
    "                    proportions.append(proportion)\n",
    "            if proportions:\n",
    "                avg_proportion = sum(proportions) / len(proportions)\n",
    "                avg_proportions.append(avg_proportion)\n",
    "    \n",
    "            avg_proportion = round(avg_proportion.item(), 4)\n",
    "\n",
    "            print(f\"Average Energy-Based Pointing Game Proportion (Negative): {avg_proportion}\", flush=True)\n",
    "\n",
    "\n",
    "    final_avg_prop = sum(avg_proportions) / len(avg_proportions)\n",
    "    final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "    \n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Negative) over all folds: {final_avg_prop}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Recall across all folds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.3877\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.2465, Count: 260\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.4267, Count: 942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.2955\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.2075, Count: 319\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.3273, Count: 883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.7817\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.7945, Count: 391\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.7755, Count: 812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.7608\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.7397, Count: 306\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.768, Count: 897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.7827\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.8027, Count: 342\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.7747, Count: 860\n",
      "\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.6017\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.5582\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.6144\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from libraries.energyPointGame import energy_point_game, energy_point_game_recall\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "\n",
    "BASE_PATH = \"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/trained_models/\"\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "avg_proportions = []\n",
    "avg_proportions_incorrect = []\n",
    "avg_proportions_correct = []\n",
    "\n",
    "for fold in range(1,6):\n",
    "    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/Transformer_Conv_BCos/light_oversamp/seed_0/pneumonia_detection_model_transformer_bestf1_{fold}.pth\"\n",
    "    split = splits[fold-1] # fold selection\n",
    "    val_idx = split[1]  # Only use the validation indices from the first fold\n",
    "    val_data = data_splits.iloc[val_idx]\n",
    "    val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "    proportions = []\n",
    "    proportions_correct = []\n",
    "    proportions_incorrect = []\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    \n",
    "    model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "    model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "    #model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "    #model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, patient_ids in val_loader:\n",
    "            #images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            six_channel_images = []\n",
    "            for img_tensor in images:\n",
    "                numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil_image = Image.fromarray(numpy_image)\n",
    "                transformed_image = model.transform(pil_image)\n",
    "                six_channel_images.append(transformed_image)\n",
    "                \n",
    "            six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "            \n",
    "            \n",
    "            for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                if not filtered_rows.empty: \n",
    "                    image = image[None]\n",
    "                    output = model(image)\n",
    "                    #prediction = torch.argmax(output, dim=1)\n",
    "                    expl = model.explain(image)\n",
    "                    prediction = expl['prediction']\n",
    "                    contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                    proportion_total = 0.0\n",
    "                    num_boxes = len(filtered_rows)  \n",
    "                    for _, row in filtered_rows.iterrows():\n",
    "                        x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                        coordinates_list = [x, y, x + width, y + height]\n",
    "                        coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                        ebpg_result = energy_point_game_recall(coordinates_tensor, contribution_map)\n",
    "                        proportion_total += ebpg_result\n",
    "                        \n",
    "                    average_proportion = proportion_total / num_boxes if num_boxes > 0 else 0.0\n",
    "                    proportions.append(average_proportion)\n",
    "                    if prediction == 1:\n",
    "                        proportions_correct.append(average_proportion)\n",
    "                        count_correct = count_correct + 1\n",
    "                    else:\n",
    "                        proportions_incorrect.append(average_proportion)\n",
    "                        count_incorrect = count_incorrect + 1\n",
    "    if proportions:\n",
    "        avg_proportion = sum(proportions) / len(proportions)\n",
    "        avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "        avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "        avg_proportions.append(avg_proportion)\n",
    "        avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "        avg_proportions_correct.append(avg_proportion_correct)\n",
    "\n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "final_avg_prop = sum(avg_proportions) / len(avg_proportions) \n",
    "final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "\n",
    "final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "print()\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
