{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-based Pointing Game\n",
    "\n",
    "Cell directly below:\n",
    "- iterates one model (e.g. best performing fold 1 model of B-Cos ResNet50)\n",
    "- determines Energy-based pointing game of the fold fitting to the model (validation set) - first fold model focuses on validation set of first fold\n",
    "- calculates average energy-based pointing game result of the validation set\n",
    "- Results: higher proportion (result of energy-based pointing game) on correctly classified images\n",
    "\n",
    "Second Cell below:\n",
    "- prints image with corresponding explanation image\n",
    "- executes Energy-based pointing game on one image - giving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG (Precision) for a singular model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion: 0.1046\n",
      "Average Energy-Based Pointing Game Proportion of Incorrectly Classified Images: 0.0785, Count: 412\n",
      "Average Energy-Based Pointing Game Proportion of Correctly Classified Images: 0.1182, Count: 790\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''from dataset.pneumonia_dataset import PneumoniaDataset'''\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "#model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\Transformer_Conv_BCos\\no_nosamp\\seed_0\\pneumonia_detection_model_transformer_bcos_bestf1_1.pth\"\n",
    "# model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\Pneumonia\\ResNet50_BCos\\light_oversamp\\seed_0\\pneumonia_detection_model_resnet_bestf1_1.pth\"\n",
    "model_path = r\"C:\\Users\\Admin\\Downloads\\BCosResNet50_test\\seed_0_1\\pneumonia_detection_model_resnet_bestf1_1.pth\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "#model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "# Modify model and load model\n",
    "'''model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)        \n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "'''\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "\n",
    "\n",
    "def energy_point_game_debug(coords, contrib_map):\n",
    "    num_positive = (contrib_map > 0).sum().item()\n",
    "    num_negative = (contrib_map < 0).sum().item()\n",
    "    \n",
    "    print(f\"Bounding Box Coordinates: {coords.tolist()}\")\n",
    "    print(f\"Contribution Map Min/Max: {contrib_map.min().item()}, {contrib_map.max().item()}\")\n",
    "    print(f\"Positive Values Count: {num_positive}, Negative Values Count: {num_negative}\")\n",
    "    \n",
    "    result = energy_point_game(coords, contrib_map)\n",
    "    print(f\"Energy Point Game Score: {result}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()# Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "proportions = []\n",
    "proportions_correct = []\n",
    "proportions_incorrect = []\n",
    "count_correct = 0\n",
    "count_incorrect = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, patient_ids in val_loader:\n",
    "        #images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        six_channel_images = []\n",
    "        for img_tensor in images:\n",
    "            numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(numpy_image)\n",
    "            transformed_image = model.transform(pil_image)\n",
    "            six_channel_images.append(transformed_image)\n",
    "            \n",
    "        six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "        \n",
    "        for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "            filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "            if not filtered_rows.empty: \n",
    "                image = image[None]\n",
    "                output = model(image)\n",
    "                #print(image.shape)\n",
    "                prediction = torch.argmax(output, dim=1)\n",
    "                expl = model.explain(image)\n",
    "                #prediction = expl['prediction']\n",
    "                contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                #contribution_map = torch.clamp(contribution_map, min=0)\n",
    "                contribution_map[contribution_map<0] = 0  \n",
    "                proportion = 0.0\n",
    "                for _, row in filtered_rows.iterrows():\n",
    "                    x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                    #coordinates_list = [y, x, y + height,x + width]\n",
    "                    coordinates_list = [x, y, x + width, y + height]\n",
    "                    coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                    ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                    proportion += ebpg_result\n",
    "                    \n",
    "                proportions.append(proportion)\n",
    "                if prediction == 1:\n",
    "                    proportions_correct.append(proportion)\n",
    "                    count_correct = count_correct + 1\n",
    "                else:\n",
    "                    proportions_incorrect.append(proportion)\n",
    "                    count_incorrect = count_incorrect + 1\n",
    "                \n",
    "\n",
    "if proportions:\n",
    "    avg_proportion = sum(proportions) / len(proportions)\n",
    "    avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "    avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "    \n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion: {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid proportions were found.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG (Precision) across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1654\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0898, Count: 510\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2212, Count: 692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1551\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.1085, Count: 462\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.1841, Count: 740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1466\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0693, Count: 387\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.1832, Count: 816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1617\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0854, Count: 521\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2199, Count: 682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1501\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.113, Count: 477\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.1745, Count: 725\n",
      "\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.1558\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.0932\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.1966\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from libraries.energyPointGame import energy_point_game, energy_point_game_recall\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "avg_proportions = []\n",
    "avg_proportions_incorrect = []\n",
    "avg_proportions_correct = []\n",
    "\n",
    "#    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_FLC/no_nosamp/seed_0/pneumonia_detection_model_resnet_bestf1_{fold}.pth\"\n",
    "\n",
    "for fold in range(1,6):\n",
    "    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/Transformer_Conv_BCos/no_nosamp/seed_0/pneumonia_detection_model_transformer_bestf1_{fold}.pth\"\n",
    "    split = splits[fold-1] # fold selection\n",
    "    val_idx = split[1]  # Only use the validation indices from the first fold\n",
    "    val_data = data_splits.iloc[val_idx]\n",
    "    val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "    proportions = []\n",
    "    proportions_correct = []\n",
    "    proportions_incorrect = []\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    \n",
    "    model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "    model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "    #model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "    #model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=1.5, transpose=True)\n",
    "    #model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=1.5, transpose=False)\n",
    "\n",
    "    #model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=1.5, transpose=True)\n",
    "    #model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=1.5, transpose=False)\n",
    "\n",
    "    #model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=1.5, transpose=True)\n",
    "    #model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=1.5, transpose=False)    \n",
    "    #model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, patient_ids in val_loader:\n",
    "            #images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            six_channel_images = []\n",
    "            for img_tensor in images:\n",
    "                numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil_image = Image.fromarray(numpy_image)\n",
    "                transformed_image = model.transform(pil_image)\n",
    "                six_channel_images.append(transformed_image)\n",
    "                \n",
    "            six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "            \n",
    "            \n",
    "            for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                if not filtered_rows.empty: \n",
    "                    image = image[None]\n",
    "                    output = model(image)\n",
    "                    #prediction = torch.argmax(output, dim=1)\n",
    "                    expl = model.explain(image)\n",
    "                    prediction = expl['prediction']\n",
    "                    contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                    contribution_map[contribution_map<0] = 0  \n",
    "                    proportion = 0.0\n",
    "                    for _, row in filtered_rows.iterrows():\n",
    "                        x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                        coordinates_list = [x, y, x + width, y + height]\n",
    "                        coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                        ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                        proportion += ebpg_result\n",
    "                        \n",
    "                    proportions.append(proportion)\n",
    "                    if prediction == 1:\n",
    "                        proportions_correct.append(proportion)\n",
    "                        count_correct = count_correct + 1\n",
    "                    else:\n",
    "                        proportions_incorrect.append(proportion)\n",
    "                        count_incorrect = count_incorrect + 1\n",
    "    if proportions:\n",
    "        avg_proportion = sum(proportions) / len(proportions)\n",
    "        avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "        avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "        avg_proportions.append(avg_proportion)\n",
    "        avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "        avg_proportions_correct.append(avg_proportion_correct)\n",
    "\n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "final_avg_prop = sum(avg_proportions) / len(avg_proportions) \n",
    "final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "\n",
    "final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "print()\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG Recall across all folds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.9983\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.9978, Count: 262\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.9984, Count: 940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.3274\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0005, Count: 314\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.443, Count: 888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.9985\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.9995, Count: 284\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.9982, Count: 919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.3015\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0106, Count: 302\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.399, Count: 901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.4073\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.2153, Count: 356\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.488, Count: 846\n",
      "\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.6066\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.4447\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.6653\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game, energy_point_game_recall\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "\n",
    "BASE_PATH = \"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/trained_models/\"\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "avg_proportions = []\n",
    "avg_proportions_incorrect = []\n",
    "avg_proportions_correct = []\n",
    "\n",
    "for fold in range(1,6):\n",
    "    #model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/Transformer_Conv_BCos/light_oversamp/seed_0/pneumonia_detection_model_transformer_bestf1_{fold}.pth\"\n",
    "    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_FLC_1.5B/light_oversamp/seed_0/pneumonia_detection_model_resnet_bestf1_{fold}.pth\"\n",
    "    split = splits[fold-1] # fold selection\n",
    "    val_idx = split[1]  # Only use the validation indices from the first fold\n",
    "    val_data = data_splits.iloc[val_idx]\n",
    "    val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "    proportions = []\n",
    "    proportions_correct = []\n",
    "    proportions_incorrect = []\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    \n",
    "    model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "    model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=1.5, transpose=True)\n",
    "    model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=1.5, transpose=False)\n",
    "\n",
    "    model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=1.5, transpose=True)\n",
    "    model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=1.5, transpose=False)\n",
    "\n",
    "    model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=1.5, transpose=True)\n",
    "    model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=1.5, transpose=False)    \n",
    "    model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "    #model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "    #model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "    #model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "    #model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, patient_ids in val_loader:\n",
    "            #images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            six_channel_images = []\n",
    "            for img_tensor in images:\n",
    "                numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil_image = Image.fromarray(numpy_image)\n",
    "                transformed_image = model.transform(pil_image)\n",
    "                six_channel_images.append(transformed_image)\n",
    "                \n",
    "            six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "            \n",
    "            \n",
    "            for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                if not filtered_rows.empty: \n",
    "                    image = image[None]\n",
    "                    output = model(image)\n",
    "                    #prediction = torch.argmax(output, dim=1)\n",
    "                    expl = model.explain(image)\n",
    "                    prediction = expl['prediction']\n",
    "                    contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                    proportion_total = 0.0\n",
    "                    num_boxes = len(filtered_rows)  \n",
    "                    for _, row in filtered_rows.iterrows():\n",
    "                        x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                        coordinates_list = [x, y, x + width, y + height]\n",
    "                        coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                        ebpg_result = energy_point_game_recall(coordinates_tensor, contribution_map)\n",
    "                        proportion_total += ebpg_result\n",
    "                        \n",
    "                    average_proportion = proportion_total / num_boxes if num_boxes > 0 else 0.0\n",
    "                    proportions.append(average_proportion)\n",
    "                    if prediction == 1:\n",
    "                        proportions_correct.append(average_proportion)\n",
    "                        count_correct = count_correct + 1\n",
    "                    else:\n",
    "                        proportions_incorrect.append(average_proportion)\n",
    "                        count_incorrect = count_incorrect + 1\n",
    "    if proportions:\n",
    "        avg_proportion = sum(proportions) / len(proportions)\n",
    "        avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "        avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "        avg_proportions.append(avg_proportion)\n",
    "        avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "        avg_proportions_correct.append(avg_proportion_correct)\n",
    "\n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "final_avg_prop = sum(avg_proportions) / len(avg_proportions) \n",
    "final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "\n",
    "final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "print()\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG across all seeds for 'Precision' or 'normal EPG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing seed 0\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1935\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0827, Count: 260\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2241, Count: 942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1622\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0894, Count: 319\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.1885, Count: 883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1727\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0984, Count: 391\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2085, Count: 812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1441\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0533, Count: 306\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.175, Count: 897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1887\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.075, Count: 342\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2339, Count: 860\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.1722\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.0798\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.206\n",
      "\n",
      "========================================\n",
      "Processing seed 1\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1839\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0925, Count: 288\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2127, Count: 914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1536\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0533, Count: 284\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.1846, Count: 918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1701\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0886, Count: 379\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2075, Count: 824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.194\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.1123, Count: 300\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2212, Count: 903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1632\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0777, Count: 295\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.191, Count: 907\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.173\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.0849\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.2034\n",
      "\n",
      "\n",
      "=== Final Aggregate Statistics ===\n",
      "Overall EPG (Positive): 0.1726 ± 0.0004\n",
      "EPG Incorrect Classifications: 0.0824 ± 0.0026\n",
      "EPG Correct Classifications: 0.2047 ± 0.0013\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game, energy_point_game_recall\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "\n",
    "\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()# Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "seeds = [0, 1]\n",
    "all_results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Processing seed {seed}\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    avg_proportions = []\n",
    "    avg_proportions_incorrect = []\n",
    "    avg_proportions_correct = []\n",
    "\n",
    "    for fold in range(1,6):\n",
    "        model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/Transformer_Conv_BCos/light_oversamp/seed_{seed}/pneumonia_detection_model_transformer_bestf1_{fold}.pth\"        \n",
    "        split = splits[fold-1] # fold selection\n",
    "        val_idx = split[1] \n",
    "        val_data = data_splits.iloc[val_idx]\n",
    "        val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "        proportions = []\n",
    "        proportions_correct = []\n",
    "        proportions_incorrect = []\n",
    "        count_correct = 0\n",
    "        count_incorrect = 0\n",
    "        \n",
    "        model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "        model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "\n",
    "        state_dict = torch.load(model_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels, patient_ids in val_loader:\n",
    "                #images, labels = images.to(device), labels.to(device)\n",
    "                labels = labels.to(device)\n",
    "                six_channel_images = []\n",
    "                for img_tensor in images:\n",
    "                    numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                    pil_image = Image.fromarray(numpy_image)\n",
    "                    transformed_image = model.transform(pil_image)\n",
    "                    six_channel_images.append(transformed_image)\n",
    "                    \n",
    "                six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "                \n",
    "                for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                    filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                    if not filtered_rows.empty: \n",
    "                        image = image[None]\n",
    "                        output = model(image)\n",
    "                        #prediction = torch.argmax(output, dim=1)\n",
    "                        expl = model.explain(image)\n",
    "                        prediction = expl['prediction']\n",
    "                        contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                        contribution_map[contribution_map<0] = 0  \n",
    "                        proportion = 0.0\n",
    "                        for _, row in filtered_rows.iterrows():\n",
    "                            x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                            coordinates_list = [x, y, x + width, y + height]\n",
    "                            coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                            ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                            proportion += ebpg_result\n",
    "                            \n",
    "                        proportions.append(proportion)\n",
    "                        if prediction == 1:\n",
    "                            proportions_correct.append(proportion)\n",
    "                            count_correct = count_correct + 1\n",
    "                        else:\n",
    "                            proportions_incorrect.append(proportion)\n",
    "                            count_incorrect = count_incorrect + 1\n",
    "        if proportions:\n",
    "            avg_proportion = sum(proportions) / len(proportions)\n",
    "            avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "            avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "            avg_proportions.append(avg_proportion)\n",
    "            avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "            avg_proportions_correct.append(avg_proportion_correct)\n",
    "\n",
    "        avg_proportion = round(avg_proportion.item(), 4)\n",
    "        avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "        avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "        \n",
    "        print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "        print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "        print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "    final_avg_prop = sum(avg_proportions) / len(avg_proportions) \n",
    "    final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "    final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "\n",
    "    final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "    final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "    final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "        \n",
    "        # Store results per fold with seed information\n",
    "    fold_results = {\n",
    "        'seed': seed,\n",
    "        'fold': fold,\n",
    "        'avg_proportion': final_avg_prop,\n",
    "        'avg_incorrect': final_avg_prop_incorrect,\n",
    "        'avg_correct': final_avg_prop_correct,\n",
    "    }\n",
    "    all_results.append(fold_results)\n",
    "\n",
    "# Calculate aggregate statistics\n",
    "def calculate_stats(values):\n",
    "    return f\"{np.mean(values):.4f} ± {np.std(values):.4f}\"\n",
    "\n",
    "final_proportions = [r['avg_proportion'] for r in all_results]\n",
    "final_incorrect = [r['avg_incorrect'] for r in all_results]\n",
    "final_correct = [r['avg_correct'] for r in all_results]\n",
    "\n",
    "print(\"\\n\\n=== Final Aggregate Statistics ===\")\n",
    "print(f\"Overall EPG (Positive): {calculate_stats(final_proportions)}\")\n",
    "print(f\"EPG Incorrect Classifications: {calculate_stats(final_incorrect)}\")\n",
    "print(f\"EPG Correct Classifications: {calculate_stats(final_correct)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall-based calculation of EPG across all seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing seed 0\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.3877\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.2465, Count: 260\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.4267, Count: 942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.2955\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.2075, Count: 319\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.3273, Count: 883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.7817\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.7945, Count: 391\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.7755, Count: 812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.7608\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.7397, Count: 306\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.768, Count: 897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.7827\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.8027, Count: 342\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.7747, Count: 860\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.6017\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.5582\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.6144\n",
      "\n",
      "========================================\n",
      "Processing seed 1\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.5762\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.3347, Count: 288\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.6523, Count: 914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.7242\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.5553, Count: 284\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.7764, Count: 918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.7872\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.7944, Count: 379\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.7838, Count: 824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.3045\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.2103, Count: 300\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.3359, Count: 903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.7728\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.7827, Count: 295\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.7695, Count: 907\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.633\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.5355\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.6636\n",
      "\n",
      "\n",
      "=== Final Aggregate Statistics ===\n",
      "Overall EPG (Positive): 0.6174 ± 0.0156\n",
      "EPG Incorrect Classifications: 0.5469 ± 0.0114\n",
      "EPG Correct Classifications: 0.6390 ± 0.0246\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game, energy_point_game_recall\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "\n",
    "\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()# Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "seeds = [0, 1]\n",
    "all_results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Processing seed {seed}\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    avg_proportions = []\n",
    "    avg_proportions_incorrect = []\n",
    "    avg_proportions_correct = []\n",
    "\n",
    "    for fold in range(1,6):\n",
    "        model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/Transformer_Conv_BCos/light_oversamp/seed_{seed}/pneumonia_detection_model_transformer_bestf1_{fold}.pth\"        \n",
    "        split = splits[fold-1] # fold selection\n",
    "        val_idx = split[1] \n",
    "        val_data = data_splits.iloc[val_idx]\n",
    "        val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "        proportions = []\n",
    "        proportions_correct = []\n",
    "        proportions_incorrect = []\n",
    "        count_correct = 0\n",
    "        count_incorrect = 0\n",
    "        model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "        model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "        #model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "        #model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=1.5, transpose=True)\n",
    "        #model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=1.5, transpose=False)\n",
    "\n",
    "        #model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=1.5, transpose=True)\n",
    "        #model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=1.5, transpose=False)\n",
    "\n",
    "        #model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=1.5, transpose=True)\n",
    "        #model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=1.5, transpose=False)    \n",
    "\n",
    "        #model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "\n",
    "        state_dict = torch.load(model_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels, patient_ids in val_loader:\n",
    "                #images, labels = images.to(device), labels.to(device)\n",
    "                labels = labels.to(device)\n",
    "                six_channel_images = []\n",
    "                for img_tensor in images:\n",
    "                    numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                    pil_image = Image.fromarray(numpy_image)\n",
    "                    transformed_image = model.transform(pil_image)\n",
    "                    six_channel_images.append(transformed_image)\n",
    "                    \n",
    "                six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "                \n",
    "                for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                    filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                    if not filtered_rows.empty: \n",
    "                        image = image[None]\n",
    "                        output = model(image)\n",
    "                        #prediction = torch.argmax(output, dim=1)\n",
    "                        expl = model.explain(image)\n",
    "                        prediction = expl['prediction']\n",
    "                        contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                        proportion_total = 0.0\n",
    "                        num_boxes = len(filtered_rows) \n",
    "                        for _, row in filtered_rows.iterrows():\n",
    "                            x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                            coordinates_list = [x, y, x + width, y + height]\n",
    "                            coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                            ebpg_result = energy_point_game_recall(coordinates_tensor, contribution_map)\n",
    "                            proportion_total += ebpg_result\n",
    "                        \n",
    "                        average_proportion = proportion_total / num_boxes if num_boxes > 0 else 0.0\n",
    "                        proportions.append(average_proportion)   \n",
    "                        if prediction == 1:\n",
    "                            proportions_correct.append(average_proportion)\n",
    "                            count_correct = count_correct + 1\n",
    "                        else:\n",
    "                            proportions_incorrect.append(average_proportion)\n",
    "                            count_incorrect = count_incorrect + 1\n",
    "        if proportions:\n",
    "            avg_proportion = sum(proportions) / len(proportions)\n",
    "            avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "            avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "            avg_proportions.append(avg_proportion)\n",
    "            avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "            avg_proportions_correct.append(avg_proportion_correct)\n",
    "\n",
    "        avg_proportion = round(avg_proportion.item(), 4)\n",
    "        avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "        avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "        \n",
    "        print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "        print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "        print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "    final_avg_prop = sum(avg_proportions) / len(avg_proportions) \n",
    "    final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "    final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "\n",
    "    final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "    final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "    final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "        \n",
    "        # Store results per fold with seed information\n",
    "    fold_results = {\n",
    "        'seed': seed,\n",
    "        'fold': fold,\n",
    "        'avg_proportion': final_avg_prop,\n",
    "        'avg_incorrect': final_avg_prop_incorrect,\n",
    "        'avg_correct': final_avg_prop_correct,\n",
    "    }\n",
    "    all_results.append(fold_results)\n",
    "\n",
    "# Calculate aggregate statistics\n",
    "def calculate_stats(values):\n",
    "    return f\"{np.mean(values):.4f} ± {np.std(values):.4f}\"\n",
    "\n",
    "final_proportions = [r['avg_proportion'] for r in all_results]\n",
    "final_incorrect = [r['avg_incorrect'] for r in all_results]\n",
    "final_correct = [r['avg_correct'] for r in all_results]\n",
    "\n",
    "print(\"\\n\\n=== Final Aggregate Statistics ===\")\n",
    "print(f\"Overall EPG (Positive): {calculate_stats(final_proportions)}\")\n",
    "print(f\"EPG Incorrect Classifications: {calculate_stats(final_incorrect)}\")\n",
    "print(f\"EPG Correct Classifications: {calculate_stats(final_correct)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Threshold Precision from 0.1 to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0\n",
      "Average Energy-Based Pointing Game Proportion (Positive): 0.3416\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0977, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.4901, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 1\n",
      "Average Energy-Based Pointing Game Proportion (Positive): 0.4097\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0986, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.5989, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 2\n",
      "Average Energy-Based Pointing Game Proportion (Positive): 0.4579\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.1001, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.6756, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 3\n",
      "Average Energy-Based Pointing Game Proportion (Positive): 0.4891\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.097, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.7277, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 4\n",
      "Average Energy-Based Pointing Game Proportion (Positive): 0.5086\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0935, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.7611, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 5\n",
      "Average Energy-Based Pointing Game Proportion (Positive): 0.5206\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0907, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.7821, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 6\n",
      "Average Energy-Based Pointing Game Proportion (Positive): 0.5272\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0873, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.7948, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 7\n",
      "Average Energy-Based Pointing Game Proportion (Positive): 0.5323\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0875, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.8028, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 8\n",
      "Average Energy-Based Pointing Game Proportion (Positive): 0.533\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0868, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.8045, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 9\n",
      "Average Energy-Based Pointing Game Proportion (Positive): 0.5315\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0871, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.8019, Count: 748\n",
      "\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.4852\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.0926\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.7239\n",
      "[tensor(0.3416), tensor(0.4097), tensor(0.4579), tensor(0.4891), tensor(0.5086), tensor(0.5206), tensor(0.5272), tensor(0.5323), tensor(0.5330), tensor(0.5315)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game, energy_point_game_recall\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "avg_proportions = []\n",
    "avg_proportions_incorrect = []\n",
    "avg_proportions_correct = []\n",
    "\n",
    "#    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_FLC/no_nosamp/seed_0/pneumonia_detection_model_resnet_bestf1_{fold}.pth\"\n",
    "\n",
    "for threshold_multiplier in range(0,10):\n",
    "    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_FLC/no_nosamp/seed_1/pneumonia_detection_model_resnet_bestf1_3.pth\"\n",
    "    split = splits[2] # fold selection\n",
    "    val_idx = split[1]  # Only use the validation indices from the first fold\n",
    "    val_data = data_splits.iloc[val_idx]\n",
    "    val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "    proportions = []\n",
    "    proportions_correct = []\n",
    "    proportions_incorrect = []\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    \n",
    "    model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "    model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "    model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "    model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "    model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "    model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "    model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)    \n",
    "        \n",
    "    model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, patient_ids in val_loader:\n",
    "            #images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            six_channel_images = []\n",
    "            for img_tensor in images:\n",
    "                numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil_image = Image.fromarray(numpy_image)\n",
    "                transformed_image = model.transform(pil_image)\n",
    "                six_channel_images.append(transformed_image)\n",
    "                \n",
    "            six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "            \n",
    "            \n",
    "            for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                if not filtered_rows.empty: \n",
    "                    image = image[None]\n",
    "                    output = model(image)\n",
    "                    #prediction = torch.argmax(output, dim=1)\n",
    "                    expl = model.explain(image)\n",
    "                    prediction = expl['prediction']\n",
    "                    contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                    contribution_map[contribution_map<0] = 0  \n",
    "                    proportion = 0.0\n",
    "                    for _, row in filtered_rows.iterrows():\n",
    "                        x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                        coordinates_list = [x, y, x + width, y + height]\n",
    "                        coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                        ebpg_result = energy_point_game(coordinates_tensor, contribution_map, threshold=0.1*threshold_multiplier)\n",
    "                        proportion += ebpg_result\n",
    "                        \n",
    "                    proportions.append(proportion)\n",
    "                    if prediction == 1:\n",
    "                        proportions_correct.append(proportion)\n",
    "                        count_correct = count_correct + 1\n",
    "                    else:\n",
    "                        proportions_incorrect.append(proportion)\n",
    "                        count_incorrect = count_incorrect + 1\n",
    "    if proportions:\n",
    "        avg_proportion = sum(proportions) / len(proportions)\n",
    "        avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "        avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "        avg_proportions.append(avg_proportion)\n",
    "        avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "        avg_proportions_correct.append(avg_proportion_correct)\n",
    "\n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Threshold: {threshold_multiplier}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "final_avg_prop = sum(avg_proportions) / len(avg_proportions) \n",
    "final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "\n",
    "final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "print()\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "\n",
    "print(avg_proportions)\n",
    "# Plotting the result\n",
    "thresholds = [0.1 * i for i in range(len(avg_proportions))]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(thresholds, avg_proportions, marker='o', label='All')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Average EPG Proportion')\n",
    "plt.title('EPG Proportion vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Threshold Recall from 0.1 to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.9595\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.9233, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.9815, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.6571\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.4503, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.7828, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.4541\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.2137, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.6004, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.3043\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0946, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.432, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1922\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0385, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2857, Count: 748\n",
      "\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.5134\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.344\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.6165\n",
      "[tensor(0.9595), tensor(0.6571), tensor(0.4541), tensor(0.3043), tensor(0.1922)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game, energy_point_game_recall\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "avg_proportions = []\n",
    "avg_proportions_incorrect = []\n",
    "avg_proportions_correct = []\n",
    "\n",
    "for threshold_multiplier in range(0,5):\n",
    "    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_FLC/no_nosamp/seed_1/pneumonia_detection_model_resnet_bestf1_3.pth\"\n",
    "    split = splits[2] # fold selection\n",
    "    val_idx = split[1]  # Only use the validation indices from the first fold\n",
    "    val_data = data_splits.iloc[val_idx]\n",
    "    val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "    proportions = []\n",
    "    proportions_correct = []\n",
    "    proportions_incorrect = []\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    \n",
    "    model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "    model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "    model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "    model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "    model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "    model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "    model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)    \n",
    "        \n",
    "    model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "    #model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "    #model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "    #model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "    #model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, patient_ids in val_loader:\n",
    "            #images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            six_channel_images = []\n",
    "            for img_tensor in images:\n",
    "                numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil_image = Image.fromarray(numpy_image)\n",
    "                transformed_image = model.transform(pil_image)\n",
    "                six_channel_images.append(transformed_image)\n",
    "                \n",
    "            six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "            \n",
    "            \n",
    "            for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                if not filtered_rows.empty: \n",
    "                    image = image[None]\n",
    "                    output = model(image)\n",
    "                    #prediction = torch.argmax(output, dim=1)\n",
    "                    expl = model.explain(image)\n",
    "                    prediction = expl['prediction']\n",
    "                    contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                    proportion_total = 0.0\n",
    "                    num_boxes = len(filtered_rows)  \n",
    "                    for _, row in filtered_rows.iterrows():\n",
    "                        x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                        coordinates_list = [x, y, x + width, y + height]\n",
    "                        coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                        ebpg_result = energy_point_game_recall(coordinates_tensor, contribution_map, threshold=0.1*threshold_multiplier)\n",
    "                        proportion_total += ebpg_result\n",
    "                        \n",
    "                    average_proportion = proportion_total / num_boxes if num_boxes > 0 else 0.0\n",
    "                    proportions.append(average_proportion)\n",
    "                    if prediction == 1:\n",
    "                        proportions_correct.append(average_proportion)\n",
    "                        count_correct = count_correct + 1\n",
    "                    else:\n",
    "                        proportions_incorrect.append(average_proportion)\n",
    "                        count_incorrect = count_incorrect + 1\n",
    "    if proportions:\n",
    "        avg_proportion = sum(proportions) / len(proportions)\n",
    "        avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "        avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "        avg_proportions.append(avg_proportion)\n",
    "        avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "        avg_proportions_correct.append(avg_proportion_correct)\n",
    "\n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "final_avg_prop = sum(avg_proportions) / len(avg_proportions) \n",
    "final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "\n",
    "final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "print()\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "thresholds = [0.1 * i for i in range(len(avg_proportions))]\n",
    "\n",
    "print(avg_proportions)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(thresholds, avg_proportions, marker='o', label='All')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Average EPG Proportion')\n",
    "plt.title('EPG Proportion vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
