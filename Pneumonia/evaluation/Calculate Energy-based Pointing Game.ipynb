{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-based Pointing Game\n",
    "\n",
    "Cell directly below:\n",
    "- iterates one model (e.g. best performing fold 1 model of B-Cos ResNet50)\n",
    "- determines Energy-based pointing game of the fold fitting to the model (validation set) - first fold model focuses on validation set of first fold\n",
    "- calculates average energy-based pointing game result of the validation set\n",
    "- Results: higher proportion (result of energy-based pointing game) on correctly classified images\n",
    "\n",
    "Second Cell below:\n",
    "- prints image with corresponding explanation image\n",
    "- executes Energy-based pointing game on one image - giving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG for Positive Contributions\n",
    "value rather low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion: 0.1046\n",
      "Average Energy-Based Pointing Game Proportion of Incorrectly Classified Images: 0.0785, Count: 412\n",
      "Average Energy-Based Pointing Game Proportion of Correctly Classified Images: 0.1182, Count: 790\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''from dataset.pneumonia_dataset import PneumoniaDataset'''\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "#model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\Transformer_Conv_BCos\\no_nosamp\\seed_0\\pneumonia_detection_model_transformer_bcos_bestf1_1.pth\"\n",
    "# model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\Pneumonia\\ResNet50_BCos\\light_oversamp\\seed_0\\pneumonia_detection_model_resnet_bestf1_1.pth\"\n",
    "model_path = r\"C:\\Users\\Admin\\Downloads\\BCosResNet50_test\\seed_0_1\\pneumonia_detection_model_resnet_bestf1_1.pth\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "#model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "# Modify model and load model\n",
    "'''model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)        \n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "'''\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "\n",
    "\n",
    "def energy_point_game_debug(coords, contrib_map):\n",
    "    num_positive = (contrib_map > 0).sum().item()\n",
    "    num_negative = (contrib_map < 0).sum().item()\n",
    "    \n",
    "    print(f\"Bounding Box Coordinates: {coords.tolist()}\")\n",
    "    print(f\"Contribution Map Min/Max: {contrib_map.min().item()}, {contrib_map.max().item()}\")\n",
    "    print(f\"Positive Values Count: {num_positive}, Negative Values Count: {num_negative}\")\n",
    "    \n",
    "    result = energy_point_game(coords, contrib_map)\n",
    "    print(f\"Energy Point Game Score: {result}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()# Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "proportions = []\n",
    "proportions_correct = []\n",
    "proportions_incorrect = []\n",
    "count_correct = 0\n",
    "count_incorrect = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, patient_ids in val_loader:\n",
    "        #images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        six_channel_images = []\n",
    "        for img_tensor in images:\n",
    "            numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(numpy_image)\n",
    "            transformed_image = model.transform(pil_image)\n",
    "            six_channel_images.append(transformed_image)\n",
    "            \n",
    "        six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "        \n",
    "        for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "            filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "            if not filtered_rows.empty: \n",
    "                image = image[None]\n",
    "                output = model(image)\n",
    "                #print(image.shape)\n",
    "                prediction = torch.argmax(output, dim=1)\n",
    "                expl = model.explain(image)\n",
    "                #prediction = expl['prediction']\n",
    "                contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                #contribution_map = torch.clamp(contribution_map, min=0)\n",
    "                contribution_map[contribution_map<0] = 0  \n",
    "                proportion = 0.0\n",
    "                for _, row in filtered_rows.iterrows():\n",
    "                    x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                    #coordinates_list = [y, x, y + height,x + width]\n",
    "                    coordinates_list = [x, y, x + width, y + height]\n",
    "                    coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                    ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                    proportion += ebpg_result\n",
    "                    \n",
    "                proportions.append(proportion)\n",
    "                if prediction == 1:\n",
    "                    proportions_correct.append(proportion)\n",
    "                    count_correct = count_correct + 1\n",
    "                else:\n",
    "                    proportions_incorrect.append(proportion)\n",
    "                    count_incorrect = count_incorrect + 1\n",
    "                \n",
    "\n",
    "if proportions:\n",
    "    avg_proportion = sum(proportions) / len(proportions)\n",
    "    avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "    avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "    \n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion: {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid proportions were found.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG for Negative Contributions - everything aside Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion: 0.8687\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''from dataset.pneumonia_dataset import PneumoniaDataset'''\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "#model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\Transformer_Conv_BCos\\no_nosamp\\seed_0\\pneumonia_detection_model_transformer_bcos_bestf1_1.pth\"\n",
    "model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\ResNet50_BCos\\light_oversamp_nonorm\\seed_0\\pneumonia_detection_model_resnet_bcos_bestf1_1.pth\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "#model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "# Modify model and load model\n",
    "'''model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)        \n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "'''\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "\n",
    "\n",
    "def energy_point_game_debug(coords, contrib_map):\n",
    "    num_positive = (contrib_map > 0).sum().item()\n",
    "    num_negative = (contrib_map < 0).sum().item()\n",
    "    \n",
    "    print(f\"Bounding Box Coordinates: {coords.tolist()}\")\n",
    "    print(f\"Contribution Map Min/Max: {contrib_map.min().item()}, {contrib_map.max().item()}\")\n",
    "    print(f\"Positive Values Count: {num_positive}, Negative Values Count: {num_negative}\")\n",
    "    \n",
    "    result = energy_point_game(coords, contrib_map)\n",
    "    print(f\"Energy Point Game Score: {result}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "proportions = []\n",
    "proportions_correct = []\n",
    "proportions_incorrect = []\n",
    "count_correct = 0\n",
    "count_incorrect = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, patient_ids in val_loader:\n",
    "        #images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        six_channel_images = []\n",
    "        for img_tensor in images:\n",
    "            numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(numpy_image)\n",
    "            transformed_image = model.transform(pil_image)\n",
    "            six_channel_images.append(transformed_image)\n",
    "            \n",
    "        six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "        \n",
    "        for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "            filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "            if not filtered_rows.empty: \n",
    "                image = image[None]\n",
    "                output = model(image)\n",
    "                #print(image.shape)\n",
    "                prediction = torch.argmax(output, dim=1)\n",
    "                expl = model.explain(image)\n",
    "                #prediction = expl['prediction']\n",
    "                contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                contribution_map[contribution_map>0] = 0  \n",
    "                proportion = 1.0\n",
    "                for _, row in filtered_rows.iterrows():\n",
    "                    x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                    coordinates_list = [x, y, x + width, y + height]\n",
    "                    coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                    ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                    proportion = proportion - ebpg_result\n",
    "                    \n",
    "                proportions.append(proportion)                \n",
    "\n",
    "if proportions:\n",
    "    avg_proportion = sum(proportions) / len(proportions)    \n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion: {avg_proportion}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid proportions were found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.2825\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.1217, Count: 450\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.3788, Count: 752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.2689\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.1505, Count: 487\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.3495, Count: 715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.3416\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0977, Count: 455\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.4901, Count: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.2952\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0241, Count: 443\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.4532, Count: 760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.3028\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.1217, Count: 328\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.3708, Count: 874\n",
      "\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.2982\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.1032\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.4085\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from libraries.energyPointGame import energy_point_game, energy_point_game_recall\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "avg_proportions = []\n",
    "avg_proportions_incorrect = []\n",
    "avg_proportions_correct = []\n",
    "\n",
    "#    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_FLC/no_nosamp/seed_0/pneumonia_detection_model_resnet_bestf1_{fold}.pth\"\n",
    "\n",
    "for fold in range(1,6):\n",
    "    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_FLC/no_nosamp/seed_1/pneumonia_detection_model_resnet_bestf1_{fold}.pth\"\n",
    "    split = splits[fold-1] # fold selection\n",
    "    val_idx = split[1]  # Only use the validation indices from the first fold\n",
    "    val_data = data_splits.iloc[val_idx]\n",
    "    val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "    proportions = []\n",
    "    proportions_correct = []\n",
    "    proportions_incorrect = []\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    \n",
    "    model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "    model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "    model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "    model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "    model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "    model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "    model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)    \n",
    "    model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, patient_ids in val_loader:\n",
    "            #images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            six_channel_images = []\n",
    "            for img_tensor in images:\n",
    "                numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil_image = Image.fromarray(numpy_image)\n",
    "                transformed_image = model.transform(pil_image)\n",
    "                six_channel_images.append(transformed_image)\n",
    "                \n",
    "            six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "            \n",
    "            \n",
    "            for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                if not filtered_rows.empty: \n",
    "                    image = image[None]\n",
    "                    output = model(image)\n",
    "                    #prediction = torch.argmax(output, dim=1)\n",
    "                    expl = model.explain(image)\n",
    "                    prediction = expl['prediction']\n",
    "                    contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                    contribution_map[contribution_map<0] = 0  \n",
    "                    proportion = 0.0\n",
    "                    for _, row in filtered_rows.iterrows():\n",
    "                        x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                        coordinates_list = [x, y, x + width, y + height]\n",
    "                        coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                        ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                        proportion += ebpg_result\n",
    "                        \n",
    "                    proportions.append(proportion)\n",
    "                    if prediction == 1:\n",
    "                        proportions_correct.append(proportion)\n",
    "                        count_correct = count_correct + 1\n",
    "                    else:\n",
    "                        proportions_incorrect.append(proportion)\n",
    "                        count_incorrect = count_incorrect + 1\n",
    "    if proportions:\n",
    "        avg_proportion = sum(proportions) / len(proportions)\n",
    "        avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "        avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "        avg_proportions.append(avg_proportion)\n",
    "        avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "        avg_proportions_correct.append(avg_proportion_correct)\n",
    "\n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "final_avg_prop = sum(avg_proportions) / len(avg_proportions) \n",
    "final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "\n",
    "final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "print()\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG (Negative) outside of Bounding Boxes across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_BCos/light_oversamp_nonorm/seed_0/pneumonia_detection_model_resnet_bcos_bestf1_1.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 106\u001b[0m\n\u001b[0;32m    104\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB-cos/B-cos-v2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet50\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    105\u001b[0m model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m NormedConv2d(\u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m2\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# code from B-cos paper reused to adjust network\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m    108\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_BCos/light_oversamp_nonorm/seed_0/pneumonia_detection_model_resnet_bcos_bestf1_1.pth'"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''from dataset.pneumonia_dataset import PneumoniaDataset'''\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "avg_proportions = []\n",
    "avg_proportions_incorrect = []\n",
    "avg_proportions_correct = []\n",
    "\n",
    "for fold in range(1,6):\n",
    "    proportions = []\n",
    "    proportions_correct = []\n",
    "    proportions_incorrect = []\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_BCos/light_oversamp_nonorm/seed_0/pneumonia_detection_model_resnet_bcos_bestf1_1.pth\" \n",
    "    model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "    model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, patient_ids in val_loader:\n",
    "            #images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            six_channel_images = []\n",
    "            for img_tensor in images:\n",
    "                numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil_image = Image.fromarray(numpy_image)\n",
    "                transformed_image = model.transform(pil_image)\n",
    "                six_channel_images.append(transformed_image)\n",
    "                \n",
    "            six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "            \n",
    "            for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                if not filtered_rows.empty: \n",
    "                    image = image[None]\n",
    "                    output = model(image)\n",
    "                    prediction = torch.argmax(output, dim=1)\n",
    "                    expl = model.explain(image)\n",
    "                    #prediction = expl['prediction']\n",
    "                    contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                    contribution_map[contribution_map>0] = 0  \n",
    "                    proportion = 1.0\n",
    "                    for _, row in filtered_rows.iterrows():\n",
    "                        x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                        coordinates_list = [x, y, x + width, y + height]\n",
    "                        coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                        ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                        proportion = proportion - ebpg_result\n",
    "                        \n",
    "                    proportions.append(proportion)\n",
    "            if proportions:\n",
    "                avg_proportion = sum(proportions) / len(proportions)\n",
    "                avg_proportions.append(avg_proportion)\n",
    "    \n",
    "            avg_proportion = round(avg_proportion.item(), 4)\n",
    "\n",
    "            print(f\"Average Energy-Based Pointing Game Proportion (Negative): {avg_proportion}\", flush=True)\n",
    "\n",
    "\n",
    "    final_avg_prop = sum(avg_proportions) / len(avg_proportions)\n",
    "    final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "    \n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Negative) over all folds: {final_avg_prop}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Recall across all folds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.9171\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.7921, Count: 421\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.9844, Count: 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.0791\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0431, Count: 338\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.0932, Count: 864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.722\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.4514, Count: 402\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.8578, Count: 801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.9526\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.8734, Count: 336\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.9833, Count: 867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.6702\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.4494, Count: 537\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.8485, Count: 665\n",
      "\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.6682\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.5219\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.7535\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game, energy_point_game_recall\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "\n",
    "BASE_PATH = \"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/trained_models/\"\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "avg_proportions = []\n",
    "avg_proportions_incorrect = []\n",
    "avg_proportions_correct = []\n",
    "\n",
    "for fold in range(1,6):\n",
    "    #model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/Transformer_Conv_BCos/light_oversamp/seed_0/pneumonia_detection_model_transformer_bestf1_{fold}.pth\"\n",
    "    model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_FLC/no_nosamp/seed_0/pneumonia_detection_model_resnet_bestf1_{fold}.pth\"\n",
    "    split = splits[fold-1] # fold selection\n",
    "    val_idx = split[1]  # Only use the validation indices from the first fold\n",
    "    val_data = data_splits.iloc[val_idx]\n",
    "    val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "    proportions = []\n",
    "    proportions_correct = []\n",
    "    proportions_incorrect = []\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    \n",
    "    model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "    model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "    model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "    model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "    model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "    model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "    model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)    \n",
    "    model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "    #model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "    #model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "    #model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "    #model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, patient_ids in val_loader:\n",
    "            #images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            six_channel_images = []\n",
    "            for img_tensor in images:\n",
    "                numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil_image = Image.fromarray(numpy_image)\n",
    "                transformed_image = model.transform(pil_image)\n",
    "                six_channel_images.append(transformed_image)\n",
    "                \n",
    "            six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "            \n",
    "            \n",
    "            for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                if not filtered_rows.empty: \n",
    "                    image = image[None]\n",
    "                    output = model(image)\n",
    "                    #prediction = torch.argmax(output, dim=1)\n",
    "                    expl = model.explain(image)\n",
    "                    prediction = expl['prediction']\n",
    "                    contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                    proportion_total = 0.0\n",
    "                    num_boxes = len(filtered_rows)  \n",
    "                    for _, row in filtered_rows.iterrows():\n",
    "                        x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                        coordinates_list = [x, y, x + width, y + height]\n",
    "                        coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                        ebpg_result = energy_point_game_recall(coordinates_tensor, contribution_map)\n",
    "                        proportion_total += ebpg_result\n",
    "                        \n",
    "                    average_proportion = proportion_total / num_boxes if num_boxes > 0 else 0.0\n",
    "                    proportions.append(average_proportion)\n",
    "                    if prediction == 1:\n",
    "                        proportions_correct.append(average_proportion)\n",
    "                        count_correct = count_correct + 1\n",
    "                    else:\n",
    "                        proportions_incorrect.append(average_proportion)\n",
    "                        count_incorrect = count_incorrect + 1\n",
    "    if proportions:\n",
    "        avg_proportion = sum(proportions) / len(proportions)\n",
    "        avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "        avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "        avg_proportions.append(avg_proportion)\n",
    "        avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "        avg_proportions_correct.append(avg_proportion_correct)\n",
    "\n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "final_avg_prop = sum(avg_proportions) / len(avg_proportions) \n",
    "final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "\n",
    "final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "print()\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG across all seeds for 'Precision' or 'normal EPG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing seed 0\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.0924\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0057, Count: 296\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.1207, Count: 906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.2301\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0855, Count: 290\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2761, Count: 912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.0834\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0051, Count: 262\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.1052, Count: 941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.2756\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.1134, Count: 270\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.3225, Count: 933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.2409\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0973, Count: 284\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2853, Count: 918\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.1845\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.0614\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.222\n",
      "\n",
      "========================================\n",
      "Processing seed 1\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.0435\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0005, Count: 253\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.055, Count: 949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.0861\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0031, Count: 285\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.1119, Count: 917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.0846\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0015, Count: 279\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.1097, Count: 924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.171\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0327, Count: 240\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2055, Count: 963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.2279\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0924, Count: 318\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.2767, Count: 884\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.1226\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.026\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.1518\n",
      "\n",
      "\n",
      "=== Final Aggregate Statistics ===\n",
      "Overall EPG (Positive): 0.1535 ± 0.0309\n",
      "EPG Incorrect Classifications: 0.0437 ± 0.0177\n",
      "EPG Correct Classifications: 0.1869 ± 0.0351\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game, energy_point_game_recall\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "\n",
    "\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()# Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "seeds = [0, 1]\n",
    "all_results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Processing seed {seed}\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    avg_proportions = []\n",
    "    avg_proportions_incorrect = []\n",
    "    avg_proportions_correct = []\n",
    "\n",
    "    for fold in range(1,6):\n",
    "        model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_BCos/light_oversamp/seed_{seed}/pneumonia_detection_model_resnet_bestf1_{fold}.pth\"        \n",
    "        split = splits[fold-1] # fold selection\n",
    "        val_idx = split[1] \n",
    "        val_data = data_splits.iloc[val_idx]\n",
    "        val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "        proportions = []\n",
    "        proportions_correct = []\n",
    "        proportions_incorrect = []\n",
    "        count_correct = 0\n",
    "        count_incorrect = 0\n",
    "        \n",
    "        model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)    \n",
    "        model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "\n",
    "        state_dict = torch.load(model_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels, patient_ids in val_loader:\n",
    "                #images, labels = images.to(device), labels.to(device)\n",
    "                labels = labels.to(device)\n",
    "                six_channel_images = []\n",
    "                for img_tensor in images:\n",
    "                    numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                    pil_image = Image.fromarray(numpy_image)\n",
    "                    transformed_image = model.transform(pil_image)\n",
    "                    six_channel_images.append(transformed_image)\n",
    "                    \n",
    "                six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "                \n",
    "                for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                    filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                    if not filtered_rows.empty: \n",
    "                        image = image[None]\n",
    "                        output = model(image)\n",
    "                        #prediction = torch.argmax(output, dim=1)\n",
    "                        expl = model.explain(image)\n",
    "                        prediction = expl['prediction']\n",
    "                        contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                        contribution_map[contribution_map<0] = 0  \n",
    "                        proportion = 0.0\n",
    "                        for _, row in filtered_rows.iterrows():\n",
    "                            x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                            coordinates_list = [x, y, x + width, y + height]\n",
    "                            coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                            ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                            proportion += ebpg_result\n",
    "                            \n",
    "                        proportions.append(proportion)\n",
    "                        if prediction == 1:\n",
    "                            proportions_correct.append(proportion)\n",
    "                            count_correct = count_correct + 1\n",
    "                        else:\n",
    "                            proportions_incorrect.append(proportion)\n",
    "                            count_incorrect = count_incorrect + 1\n",
    "        if proportions:\n",
    "            avg_proportion = sum(proportions) / len(proportions)\n",
    "            avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "            avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "            avg_proportions.append(avg_proportion)\n",
    "            avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "            avg_proportions_correct.append(avg_proportion_correct)\n",
    "\n",
    "        avg_proportion = round(avg_proportion.item(), 4)\n",
    "        avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "        avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "        \n",
    "        print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "        print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "        print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "    final_avg_prop = sum(avg_proportions) / len(avg_proportions) \n",
    "    final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "    final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "\n",
    "    final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "    final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "    final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "        \n",
    "        # Store results per fold with seed information\n",
    "    fold_results = {\n",
    "        'seed': seed,\n",
    "        'fold': fold,\n",
    "        'avg_proportion': final_avg_prop,\n",
    "        'avg_incorrect': final_avg_prop_incorrect,\n",
    "        'avg_correct': final_avg_prop_correct,\n",
    "    }\n",
    "    all_results.append(fold_results)\n",
    "\n",
    "# Calculate aggregate statistics\n",
    "def calculate_stats(values):\n",
    "    return f\"{np.mean(values):.4f} ± {np.std(values):.4f}\"\n",
    "\n",
    "final_proportions = [r['avg_proportion'] for r in all_results]\n",
    "final_incorrect = [r['avg_incorrect'] for r in all_results]\n",
    "final_correct = [r['avg_correct'] for r in all_results]\n",
    "\n",
    "print(\"\\n\\n=== Final Aggregate Statistics ===\")\n",
    "print(f\"Overall EPG (Positive): {calculate_stats(final_proportions)}\")\n",
    "print(f\"EPG Incorrect Classifications: {calculate_stats(final_incorrect)}\")\n",
    "print(f\"EPG Correct Classifications: {calculate_stats(final_correct)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall-based calculation of EPG across all seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing seed 0\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.9919\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.9837, Count: 209\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.9937, Count: 993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.9907\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.9885, Count: 314\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.9915, Count: 888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.9971\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.9967, Count: 275\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.9972, Count: 928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.2612\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.1106, Count: 330\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.3181, Count: 873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.0778\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0442, Count: 332\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.0907, Count: 870\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.6637\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.6247\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.6782\n",
      "\n",
      "========================================\n",
      "Processing seed 1\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.9999\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.9999, Count: 291\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.9999, Count: 911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.9986\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.9965, Count: 298\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.9993, Count: 904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.9945\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.9837, Count: 260\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.9974, Count: 943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.0217\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0188, Count: 331\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.0228, Count: 872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion (Positive): 0.1533\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: 0.0063, Count: 251\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: 0.1921, Count: 951\n",
      "Average Energy-Based Pointing Game Proportion (Positive) over all folds: 0.6336\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: 0.601\n",
      "Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: 0.6423\n",
      "\n",
      "\n",
      "=== Final Aggregate Statistics ===\n",
      "Overall EPG (Positive): 0.6486 ± 0.0150\n",
      "EPG Incorrect Classifications: 0.6129 ± 0.0119\n",
      "EPG Correct Classifications: 0.6603 ± 0.0180\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game, energy_point_game_recall\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "\n",
    "\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()# Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "seeds = [0, 1]\n",
    "all_results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Processing seed {seed}\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    avg_proportions = []\n",
    "    avg_proportions_incorrect = []\n",
    "    avg_proportions_correct = []\n",
    "\n",
    "    for fold in range(1,6):\n",
    "        model_path = f\"C:/Users/Admin/Documents/MasterThesis/results/Pneumonia/ResNet50_Blur/no_nosamp/seed_{seed}/pneumonia_detection_model_resnet_bcos_bestf1_{fold}.pth\"        \n",
    "        split = splits[fold-1] # fold selection\n",
    "        val_idx = split[1] \n",
    "        val_data = data_splits.iloc[val_idx]\n",
    "        val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "        proportions = []\n",
    "        proportions_correct = []\n",
    "        proportions_incorrect = []\n",
    "        count_correct = 0\n",
    "        count_incorrect = 0\n",
    "        \n",
    "        model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "        model.layer2[0].conv2 = ModifiedBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2)\n",
    "        model.layer2[0].downsample[0] = ModifiedBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2)\n",
    "\n",
    "        model.layer3[0].conv2 = ModifiedBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2)\n",
    "        model.layer3[0].downsample[0] = ModifiedBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2)\n",
    "\n",
    "        model.layer4[0].conv2 = ModifiedBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2)\n",
    "        model.layer4[0].downsample[0] = ModifiedBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2)\n",
    "        model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "\n",
    "        state_dict = torch.load(model_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels, patient_ids in val_loader:\n",
    "                #images, labels = images.to(device), labels.to(device)\n",
    "                labels = labels.to(device)\n",
    "                six_channel_images = []\n",
    "                for img_tensor in images:\n",
    "                    numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                    pil_image = Image.fromarray(numpy_image)\n",
    "                    transformed_image = model.transform(pil_image)\n",
    "                    six_channel_images.append(transformed_image)\n",
    "                    \n",
    "                six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "                \n",
    "                for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                    filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                    if not filtered_rows.empty: \n",
    "                        image = image[None]\n",
    "                        output = model(image)\n",
    "                        #prediction = torch.argmax(output, dim=1)\n",
    "                        expl = model.explain(image)\n",
    "                        prediction = expl['prediction']\n",
    "                        contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                        proportion_total = 0.0\n",
    "                        num_boxes = len(filtered_rows) \n",
    "                        for _, row in filtered_rows.iterrows():\n",
    "                            x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                            coordinates_list = [x, y, x + width, y + height]\n",
    "                            coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                            ebpg_result = energy_point_game_recall(coordinates_tensor, contribution_map)\n",
    "                            proportion_total += ebpg_result\n",
    "                        \n",
    "                        average_proportion = proportion_total / num_boxes if num_boxes > 0 else 0.0\n",
    "                        proportions.append(average_proportion)   \n",
    "                        if prediction == 1:\n",
    "                            proportions_correct.append(average_proportion)\n",
    "                            count_correct = count_correct + 1\n",
    "                        else:\n",
    "                            proportions_incorrect.append(average_proportion)\n",
    "                            count_incorrect = count_incorrect + 1\n",
    "        if proportions:\n",
    "            avg_proportion = sum(proportions) / len(proportions)\n",
    "            avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "            avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "            avg_proportions.append(avg_proportion)\n",
    "            avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "            avg_proportions_correct.append(avg_proportion_correct)\n",
    "\n",
    "        avg_proportion = round(avg_proportion.item(), 4)\n",
    "        avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "        avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "        \n",
    "        print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "        print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "        print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "    final_avg_prop = sum(avg_proportions) / len(avg_proportions) \n",
    "    final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "    final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "\n",
    "    final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "    final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "    final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "        \n",
    "        # Store results per fold with seed information\n",
    "    fold_results = {\n",
    "        'seed': seed,\n",
    "        'fold': fold,\n",
    "        'avg_proportion': final_avg_prop,\n",
    "        'avg_incorrect': final_avg_prop_incorrect,\n",
    "        'avg_correct': final_avg_prop_correct,\n",
    "    }\n",
    "    all_results.append(fold_results)\n",
    "\n",
    "# Calculate aggregate statistics\n",
    "def calculate_stats(values):\n",
    "    return f\"{np.mean(values):.4f} ± {np.std(values):.4f}\"\n",
    "\n",
    "final_proportions = [r['avg_proportion'] for r in all_results]\n",
    "final_incorrect = [r['avg_incorrect'] for r in all_results]\n",
    "final_correct = [r['avg_correct'] for r in all_results]\n",
    "\n",
    "print(\"\\n\\n=== Final Aggregate Statistics ===\")\n",
    "print(f\"Overall EPG (Positive): {calculate_stats(final_proportions)}\")\n",
    "print(f\"EPG Incorrect Classifications: {calculate_stats(final_incorrect)}\")\n",
    "print(f\"EPG Correct Classifications: {calculate_stats(final_correct)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
