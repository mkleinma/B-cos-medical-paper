{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-based Pointing Game\n",
    "\n",
    "Cell directly below:\n",
    "- iterates one model (e.g. best performing fold 1 model of B-Cos ResNet50)\n",
    "- determines Energy-based pointing game of the fold fitting to the model (validation set) - first fold model focuses on validation set of first fold\n",
    "- calculates average energy-based pointing game result of the validation set\n",
    "- Results: higher proportion (result of energy-based pointing game) on correctly classified images\n",
    "\n",
    "Second Cell below:\n",
    "- prints image with corresponding explanation image\n",
    "- executes Energy-based pointing game on one image - giving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG for Positive Contributions\n",
    "value rather low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 144\u001b[0m\n\u001b[0;32m    142\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 144\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#images, labels = images.to(device), labels.to(device)\u001b[39;49;00m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43msix_channel_images\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 114\u001b[0m, in \u001b[0;36mPneumoniaDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    111\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(image)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m--> 114\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong), patient_id\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:915\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m     )\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m--> 915\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    918\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''from dataset.pneumonia_dataset import PneumoniaDataset'''\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "#model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\Transformer_Conv_BCos\\no_nosamp\\seed_0\\pneumonia_detection_model_transformer_bcos_bestf1_1.pth\"\n",
    "model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\ResNet50_BCos\\light_oversamp_norm\\seed_0\\pneumonia_detection_model_resnet_bcos_bestf1_1.pth\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "#model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "# Modify model and load model\n",
    "'''model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)        \n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "'''\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "\n",
    "\n",
    "def energy_point_game_debug(coords, contrib_map):\n",
    "    num_positive = (contrib_map > 0).sum().item()\n",
    "    num_negative = (contrib_map < 0).sum().item()\n",
    "    \n",
    "    print(f\"Bounding Box Coordinates: {coords.tolist()}\")\n",
    "    print(f\"Contribution Map Min/Max: {contrib_map.min().item()}, {contrib_map.max().item()}\")\n",
    "    print(f\"Positive Values Count: {num_positive}, Negative Values Count: {num_negative}\")\n",
    "    \n",
    "    result = energy_point_game(coords, contrib_map)\n",
    "    print(f\"Energy Point Game Score: {result}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.490, 0.490, 0.490])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "proportions = []\n",
    "proportions_correct = []\n",
    "proportions_incorrect = []\n",
    "count_correct = 0\n",
    "count_incorrect = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, patient_ids in val_loader:\n",
    "        #images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        six_channel_images = []\n",
    "        for img_tensor in images:\n",
    "            numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(numpy_image)\n",
    "            transformed_image = model.transform(pil_image)\n",
    "            six_channel_images.append(transformed_image)\n",
    "            \n",
    "        six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "        \n",
    "        for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "            filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "            if not filtered_rows.empty: \n",
    "                image = image[None]\n",
    "                output = model(image)\n",
    "                #print(image.shape)\n",
    "                prediction = torch.argmax(output, dim=1)\n",
    "                expl = model.explain(image)\n",
    "                #prediction = expl['prediction']\n",
    "                contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                #contribution_map = torch.clamp(contribution_map, min=0)\n",
    "                contribution_map[contribution_map<0] = 0  \n",
    "                proportion = 0.0\n",
    "                for _, row in filtered_rows.iterrows():\n",
    "                    x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                    coordinates_list = [x, y, x + width, y + height]\n",
    "                    coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                    ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                    proportion += ebpg_result\n",
    "                    \n",
    "                proportions.append(proportion)\n",
    "                if prediction == 1:\n",
    "                    proportions_correct.append(proportion)\n",
    "                    count_correct = count_correct + 1\n",
    "                else:\n",
    "                    proportions_incorrect.append(proportion)\n",
    "                    count_incorrect = count_incorrect + 1\n",
    "                \n",
    "\n",
    "if proportions:\n",
    "    avg_proportion = sum(proportions) / len(proportions)\n",
    "    avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "    avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "    \n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "    avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion: {avg_proportion}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\")\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid proportions were found.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG for Negative Contributions - everything aside Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\n",
      "C:\\Users\\Admin/.cache\\torch\\hub\\B-cos_B-cos-v2_main\\bcos\\common.py:152: UserWarning: Input tensor did not require grad! Has been set automatically to True!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Energy-Based Pointing Game Proportion: 0.917\n"
     ]
    }
   ],
   "source": [
    "from libraries.energyPointGame import energy_point_game\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''from dataset.pneumonia_dataset import PneumoniaDataset'''\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_images\"\n",
    "#model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\Transformer_Conv_BCos\\no_nosamp\\seed_0\\pneumonia_detection_model_transformer_bcos_bestf1_1.pth\"\n",
    "model_path = r\"C:\\Users\\Admin\\Documents\\MasterThesis\\results\\ResNet50_BCos\\no_nosamp\\seed_0\\pneumonia_detection_model_resnet_bcos_bestf1_1.pth\"\n",
    "csv_path_splits = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#model = torch.hub.load('B-cos/B-cos-v2', 'vitc_b_patch1_14', pretrained=True)\n",
    "#model[0].linear_head.linear = BcosLinear(in_features=768, out_features=2, bias=False, b=2)\n",
    "\n",
    "model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "\n",
    "# Modify model and load model\n",
    "'''model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "\n",
    "model.layer2[0].conv2 = ModifiedFLCBcosConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer2[0].downsample[0] = ModifiedFLCBcosConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer3[0].conv2 = ModifiedFLCBcosConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer3[0].downsample[0] = ModifiedFLCBcosConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)\n",
    "\n",
    "model.layer4[0].conv2 = ModifiedFLCBcosConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), b=2, transpose=True)\n",
    "model.layer4[0].downsample[0] = ModifiedFLCBcosConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), b=2, transpose=False)        \n",
    "model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "'''\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "\n",
    "\n",
    "def energy_point_game_debug(coords, contrib_map):\n",
    "    num_positive = (contrib_map > 0).sum().item()\n",
    "    num_negative = (contrib_map < 0).sum().item()\n",
    "    \n",
    "    print(f\"Bounding Box Coordinates: {coords.tolist()}\")\n",
    "    print(f\"Contribution Map Min/Max: {contrib_map.min().item()}, {contrib_map.max().item()}\")\n",
    "    print(f\"Positive Values Count: {num_positive}, Negative Values Count: {num_negative}\")\n",
    "    \n",
    "    result = energy_point_game(coords, contrib_map)\n",
    "    print(f\"Energy Point Game Score: {result}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "proportions = []\n",
    "proportions_correct = []\n",
    "proportions_incorrect = []\n",
    "count_correct = 0\n",
    "count_incorrect = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, patient_ids in val_loader:\n",
    "        #images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        six_channel_images = []\n",
    "        for img_tensor in images:\n",
    "            numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(numpy_image)\n",
    "            transformed_image = model.transform(pil_image)\n",
    "            six_channel_images.append(transformed_image)\n",
    "            \n",
    "        six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "        \n",
    "        for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "            filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "            if not filtered_rows.empty: \n",
    "                image = image[None]\n",
    "                output = model(image)\n",
    "                #print(image.shape)\n",
    "                prediction = torch.argmax(output, dim=1)\n",
    "                expl = model.explain(image)\n",
    "                #prediction = expl['prediction']\n",
    "                contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                #contribution_map = torch.clamp(contribution_map, min=0)\n",
    "                contribution_map[contribution_map>0] = 0  \n",
    "                proportion = 1.0\n",
    "                for _, row in filtered_rows.iterrows():\n",
    "                    x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                    coordinates_list = [x, y, x + width, y + height]\n",
    "                    coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                    ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                    proportion = proportion - ebpg_result\n",
    "                    \n",
    "                proportions.append(proportion)                \n",
    "\n",
    "if proportions:\n",
    "    avg_proportion = sum(proportions) / len(proportions)    \n",
    "    avg_proportion = round(avg_proportion.item(), 4)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion: {avg_proportion}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid proportions were found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.energyPointGame import energy_point_game\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''from dataset.pneumonia_dataset import PneumoniaDataset'''\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/rsna-pneumonia-detection-challenge/stage_2_train_images\"\n",
    "csv_path_splits = r\"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/training_splits/grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "avg_proportions = []\n",
    "avg_proportions_incorrect = []\n",
    "avg_proportions_correct = []\n",
    "\n",
    "for fold in range(1,6):\n",
    "    proportions = []\n",
    "    proportions_correct = []\n",
    "    proportions_incorrect = []\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    \n",
    "    model_path = f\"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/trained_models/30_epochs_bcos_resnet50_dehb_light_oversamp_norm/seed_0/pneumonia_detection_model_resnet_bcos_bestf1_{fold}.pth\" # adjust based on model you try to check it\n",
    "    model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "    model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, patient_ids in val_loader:\n",
    "            #images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            six_channel_images = []\n",
    "            for img_tensor in images:\n",
    "                numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil_image = Image.fromarray(numpy_image)\n",
    "                transformed_image = model.transform(pil_image)\n",
    "                six_channel_images.append(transformed_image)\n",
    "                \n",
    "            six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "            \n",
    "            for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                if not filtered_rows.empty: \n",
    "                    image = image[None]\n",
    "                    output = model(image)\n",
    "                    prediction = torch.argmax(output, dim=1)\n",
    "                    expl = model.explain(image)\n",
    "                    #prediction = expl['prediction']\n",
    "                    contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                    contribution_map[contribution_map<0] = 0  \n",
    "                    proportion = 0.0\n",
    "                    for _, row in filtered_rows.iterrows():\n",
    "                        x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                        coordinates_list = [x, y, x + width, y + height]\n",
    "                        coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                        ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                        proportion += ebpg_result\n",
    "                        \n",
    "                    proportions.append(proportion)\n",
    "                    if prediction == 1:\n",
    "                        proportions_correct.append(proportion)\n",
    "                        count_correct = count_correct + 1\n",
    "                    else:\n",
    "                        proportions_incorrect.append(proportion)\n",
    "                        count_incorrect = count_incorrect + 1\n",
    "            if proportions:\n",
    "                avg_proportion = sum(proportions) / len(proportions)\n",
    "                avg_proportion_incorrect = sum(proportions_incorrect) / len(proportions_incorrect)\n",
    "                avg_proportion_correct = sum(proportions_correct) / len(proportions_correct)\n",
    "                avg_proportions.append(avg_proportion)\n",
    "                avg_proportions_incorrect.append(avg_proportion_incorrect)\n",
    "                avg_proportions_correct.append(avg_proportion_correct)\n",
    "    \n",
    "            avg_proportion = round(avg_proportion.item(), 4)\n",
    "            avg_proportion_incorrect = round(avg_proportion_incorrect.item(), 4)\n",
    "            avg_proportion_correct = round(avg_proportion_correct.item(), 4)\n",
    "\n",
    "            print(f\"Average Energy-Based Pointing Game Proportion (Positive): {avg_proportion}\")\n",
    "            print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images: {avg_proportion_incorrect}, Count: {count_incorrect}\", flush=True)\n",
    "            print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images: {avg_proportion_correct}, Count: {count_correct}\", flush=True)\n",
    "\n",
    "\n",
    "    final_avg_prop = sum(avg_proportions) / len(avg_proportions)\n",
    "    final_avg_prop_incorrect = sum(avg_proportions_incorrect) / len(avg_proportions_incorrect)\n",
    "    final_avg_prop_correct = sum(avg_proportions_correct) / len(avg_proportions_correct)\n",
    "    \n",
    "    final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "    final_avg_prop_incorrect = round(final_avg_prop_incorrect.item(), 4)\n",
    "    final_avg_prop_correct = round(final_avg_prop_correct.item(), 4)\n",
    "\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) over all folds: {final_avg_prop}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Incorrectly Classified Images over all folds: {final_avg_prop_incorrect}\", flush=True)\n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Positive) of Correctly Classified Images over all folds: {final_avg_prop_correct}\", flush=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EPG (Negative) outside of Bounding Boxes across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.energyPointGame import energy_point_game\n",
    "from libraries.ImageUtil import find_original_image_name\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "'''from dataset.pneumonia_dataset import PneumoniaDataset'''\n",
    "from dataset.augmentations import no_augmentations\n",
    "from libraries.bcosconv2d import NormedConv2d\n",
    "#from blurpool.blur_bcosconv2d import ModifiedBcosConv2d\n",
    "from pooling.flc_bcosconv2d import ModifiedFLCBcosConv2d\n",
    "from libraries.bcoslinear import BcosLinear\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "original_width, original_height = 1024, 1024\n",
    "explanation_width, explanation_height = 224, 224\n",
    "\n",
    "image_folder = r\"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/rsna-pneumonia-detection-challenge/stage_2_train_images\"\n",
    "csv_path_splits = r\"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/training_splits/grouped_data.csv\"\n",
    "csv_path = r\"C:\\Users\\Admin\\Documents\\rsna-pneumonia-detection-challenge\\stage_2_train_labels.csv\"\n",
    "splits_path = r\"G:\\Meine Ablage\\Universität\\Master Thesis\\Pneumonia\\training\\splits\\splits_balanced_fix.pkl\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data_splits = pd.read_csv(csv_path_splits)\n",
    "\n",
    "with open(splits_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# Loop over whole validation set of first fold \n",
    "first_split = splits[0] # fold selection\n",
    "val_idx = first_split[1]  # Only use the validation indices from the first fold\n",
    "val_data = data_splits.iloc[val_idx]\n",
    "\n",
    "### alternative in new models\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['patientId']}.dcm\")\n",
    "        label = row['Target']\n",
    "        patient_id = row['patientId']\n",
    "\n",
    "        # Load DICOM file and process it into RGB format\n",
    "        dicom = pydicom.dcmread(image_path)\n",
    "        image = dicom.pixel_array\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), patient_id\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "\n",
    "''' \n",
    "transform = no_augmentations() \n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "\n",
    "'''\n",
    "val_dataset = PneumoniaDataset(val_data, image_folder, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "scale_x = explanation_width / original_width\n",
    "scale_y = explanation_height / original_height\n",
    "\n",
    "avg_proportions = []\n",
    "avg_proportions_incorrect = []\n",
    "avg_proportions_correct = []\n",
    "\n",
    "for fold in range(1,6):\n",
    "    proportions = []\n",
    "    proportions_correct = []\n",
    "    proportions_incorrect = []\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    \n",
    "    model_path = f\"/pfs/work7/workspace/scratch/ma_mkleinma-thesis/trained_models/30_epochs_bcos_resnet50_dehb_light_oversamp_norm/seed_0/pneumonia_detection_model_resnet_bcos_bestf1_{fold}.pth\" # adjust based on model you try to check it\n",
    "    model = torch.hub.load('B-cos/B-cos-v2', 'resnet50', pretrained=True)\n",
    "    model.fc.linear = NormedConv2d(2048, 2, kernel_size=(1, 1), stride=(1, 1), bias=False) # code from B-cos paper reused to adjust network\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, patient_ids in val_loader:\n",
    "            #images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            six_channel_images = []\n",
    "            for img_tensor in images:\n",
    "                numpy_image = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                pil_image = Image.fromarray(numpy_image)\n",
    "                transformed_image = model.transform(pil_image)\n",
    "                six_channel_images.append(transformed_image)\n",
    "                \n",
    "            six_channel_images = torch.stack(six_channel_images).to(device)\n",
    "            \n",
    "            for image, label, patient_id in zip(six_channel_images, labels, patient_ids):\n",
    "                filtered_rows = data[(data['patientId'] == patient_id) & (data['Target'] == 1)]\n",
    "                if not filtered_rows.empty: \n",
    "                    image = image[None]\n",
    "                    output = model(image)\n",
    "                    prediction = torch.argmax(output, dim=1)\n",
    "                    expl = model.explain(image)\n",
    "                    #prediction = expl['prediction']\n",
    "                    contribution_map = expl['contribution_map'].squeeze(0).cpu()\n",
    "                    contribution_map[contribution_map>0] = 0  \n",
    "                    proportion = 1.0\n",
    "                    for _, row in filtered_rows.iterrows():\n",
    "                        x, y, width, height = round(row[\"x\"] * scale_x), round(row[\"y\"] * scale_y), round(row[\"width\"] * scale_x), round(row[\"height\"] * scale_y)\n",
    "                        coordinates_list = [x, y, x + width, y + height]\n",
    "                        coordinates_tensor = torch.tensor(coordinates_list, dtype=torch.int32)\n",
    "                        ebpg_result = energy_point_game(coordinates_tensor, contribution_map)\n",
    "                        proportion = proportion - ebpg_result\n",
    "                        \n",
    "                    proportions.append(proportion)\n",
    "            if proportions:\n",
    "                avg_proportion = sum(proportions) / len(proportions)\n",
    "                avg_proportions.append(avg_proportion)\n",
    "    \n",
    "            avg_proportion = round(avg_proportion.item(), 4)\n",
    "\n",
    "            print(f\"Average Energy-Based Pointing Game Proportion (Negative): {avg_proportion}\", flush=True)\n",
    "\n",
    "\n",
    "    final_avg_prop = sum(avg_proportions) / len(avg_proportions)\n",
    "    final_avg_prop = round(final_avg_prop.item(), 4)\n",
    "    \n",
    "    print(f\"Average Energy-Based Pointing Game Proportion (Negative) over all folds: {final_avg_prop}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
